{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54250422-05d9-4fd6-8dc1-7790ce2a0c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Create a Bedrock Runtime client in the AWS Region of your choice.\n",
    "client = boto3.client(\"bedrock-runtime\", region_name=\"us-east-1\")\n",
    "bedrock = boto3.client(service_name='bedrock', region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ec9474-7cc8-4e3e-9f5e-c0a68c851368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amazon.titan-tg1-large\n",
      "amazon.titan-image-generator-v1:0\n",
      "amazon.titan-image-generator-v1\n",
      "amazon.titan-image-generator-v2:0\n",
      "amazon.nova-premier-v1:0:8k\n",
      "amazon.nova-premier-v1:0:20k\n",
      "amazon.nova-premier-v1:0:1000k\n",
      "amazon.nova-premier-v1:0:mm\n",
      "amazon.nova-premier-v1:0\n",
      "amazon.titan-text-premier-v1:0\n",
      "amazon.nova-pro-v1:0:24k\n",
      "amazon.nova-pro-v1:0:300k\n",
      "amazon.nova-pro-v1:0\n",
      "amazon.nova-lite-v1:0:24k\n",
      "amazon.nova-lite-v1:0:300k\n",
      "amazon.nova-lite-v1:0\n",
      "amazon.nova-canvas-v1:0\n",
      "amazon.nova-reel-v1:0\n",
      "amazon.nova-reel-v1:1\n",
      "amazon.nova-micro-v1:0:24k\n",
      "amazon.nova-micro-v1:0:128k\n",
      "amazon.nova-micro-v1:0\n",
      "amazon.nova-sonic-v1:0\n",
      "amazon.titan-embed-g1-text-02\n",
      "amazon.titan-text-lite-v1:0:4k\n",
      "amazon.titan-text-lite-v1\n",
      "amazon.titan-text-express-v1:0:8k\n",
      "amazon.titan-text-express-v1\n",
      "amazon.titan-embed-text-v1:2:8k\n",
      "amazon.titan-embed-text-v1\n",
      "amazon.titan-embed-text-v2:0:8k\n",
      "amazon.titan-embed-text-v2:0\n",
      "amazon.titan-embed-image-v1:0\n",
      "amazon.titan-embed-image-v1\n",
      "stability.stable-diffusion-xl-v1:0\n",
      "stability.stable-diffusion-xl-v1\n",
      "ai21.jamba-instruct-v1:0\n",
      "ai21.jamba-1-5-large-v1:0\n",
      "ai21.jamba-1-5-mini-v1:0\n",
      "anthropic.claude-instant-v1:2:100k\n",
      "anthropic.claude-instant-v1\n",
      "anthropic.claude-v2:0:18k\n",
      "anthropic.claude-v2:0:100k\n",
      "anthropic.claude-v2:1:18k\n",
      "anthropic.claude-v2:1:200k\n",
      "anthropic.claude-v2:1\n",
      "anthropic.claude-v2\n",
      "anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "anthropic.claude-3-sonnet-20240229-v1:0\n",
      "anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "anthropic.claude-3-haiku-20240307-v1:0\n",
      "anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "anthropic.claude-3-opus-20240229-v1:0\n",
      "anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "anthropic.claude-3-7-sonnet-20250219-v1:0\n",
      "anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "anthropic.claude-opus-4-20250514-v1:0\n",
      "anthropic.claude-sonnet-4-20250514-v1:0\n",
      "cohere.command-text-v14:7:4k\n",
      "cohere.command-text-v14\n",
      "cohere.command-r-v1:0\n",
      "cohere.command-r-plus-v1:0\n",
      "cohere.command-light-text-v14:7:4k\n",
      "cohere.command-light-text-v14\n",
      "cohere.embed-english-v3:0:512\n",
      "cohere.embed-english-v3\n",
      "cohere.embed-multilingual-v3:0:512\n",
      "cohere.embed-multilingual-v3\n",
      "deepseek.r1-v1:0\n",
      "meta.llama3-8b-instruct-v1:0\n",
      "meta.llama3-70b-instruct-v1:0\n",
      "meta.llama3-1-8b-instruct-v1:0\n",
      "meta.llama3-1-70b-instruct-v1:0\n",
      "meta.llama3-2-11b-instruct-v1:0\n",
      "meta.llama3-2-90b-instruct-v1:0\n",
      "meta.llama3-2-1b-instruct-v1:0\n",
      "meta.llama3-2-3b-instruct-v1:0\n",
      "meta.llama3-3-70b-instruct-v1:0\n",
      "meta.llama4-scout-17b-instruct-v1:0\n",
      "meta.llama4-maverick-17b-instruct-v1:0\n",
      "mistral.mistral-7b-instruct-v0:2\n",
      "mistral.mixtral-8x7b-instruct-v0:1\n",
      "mistral.mistral-large-2402-v1:0\n",
      "mistral.mistral-small-2402-v1:0\n",
      "mistral.pixtral-large-2502-v1:0\n"
     ]
    }
   ],
   "source": [
    "# bedrock.list_foundation_models()\n",
    "# print the list of foundation models just using the name\n",
    "for model in bedrock.list_foundation_models()['modelSummaries']:\n",
    "    print(model['modelId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7c5b6f2-1c08-4ce8-989d-bc77fce54800",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "instruction_template = \"\"\"write an accurate 250-word gender-neutral summary of the following text without adding preamble or additional information not present in the original text, adhering to these comprehensive guidelines:\n",
    "\n",
    "1. Content Accuracy and Fidelity\n",
    "- Maintain strict adherence to the original text's core message and key points\n",
    "- Avoid introducing external information or personal interpretations\n",
    "- Preserve the chronological order and logical flow of events or ideas\n",
    "- Ensure all factual elements are accurately represented\n",
    "- Keep statistical data, dates, and numerical information exactly as presented\n",
    "\n",
    "2. Gender-Neutral Language Requirements\n",
    "- Replace gendered pronouns (he/she) with they/them or appropriate alternatives\n",
    "- Use gender-neutral job titles (firefighter instead of fireman)\n",
    "- Avoid assumptions about gender roles or stereotypes\n",
    "- Employ inclusive language that acknowledges all gender identities\n",
    "- Maintain natural sentence flow while implementing gender-neutral alternatives\n",
    "\n",
    "3. Structural Guidelines\n",
    "- Limit the summary to exactly 250 words\n",
    "- Exclude introductory statements or contextual setup\n",
    "- Begin directly with the relevant information\n",
    "- Maintain paragraph coherence and logical transitions\n",
    "- End conclusively without additional commentary\n",
    "\n",
    "4. Language and Style\n",
    "- Use clear, concise, and professional language\n",
    "- Maintain an objective tone throughout\n",
    "- Employ active voice where appropriate\n",
    "- Ensure proper grammar and punctuation\n",
    "- Preserve the original text's register and formality level\n",
    "\n",
    "5. Content Organization\n",
    "- Prioritize essential information\n",
    "- Include key supporting details\n",
    "- Maintain proper proportion of main ideas\n",
    "- Ensure logical progression of thoughts\n",
    "- Create smooth transitions between concepts\n",
    "\n",
    "6. Technical Considerations\n",
    "- Use standard English spelling and grammar\n",
    "- Maintain consistent tense throughout\n",
    "- Ensure proper noun agreement\n",
    "- Apply appropriate formatting\n",
    "- Use clear sentence structures\n",
    "\n",
    "7. Exclusion Criteria\n",
    "- No personal opinions or editorial comments\n",
    "- No additional context or background information\n",
    "- No explanatory notes or footnotes\n",
    "- No interpretative analysis\n",
    "- No concluding remarks beyond the original content\n",
    "\n",
    "8. Quality Control Measures\n",
    "- Verify all facts against the original text\n",
    "- Check for unintended gender markers\n",
    "- Ensure completeness of main ideas\n",
    "- Confirm accurate representation of relationships and causality\n",
    "- Validate precise word count\n",
    "\n",
    "9. Clarity Requirements\n",
    "- Use straightforward sentence structures\n",
    "- Avoid complex jargon unless present in original\n",
    "- Maintain clear subject-verb relationships\n",
    "- Ensure each sentence conveys complete thoughts\n",
    "- Use appropriate transitional phrases\n",
    "\n",
    "10. Professional Standards\n",
    "- Maintain academic integrity\n",
    "- Respect intellectual property\n",
    "- Preserve the original author's intent\n",
    "- Maintain appropriate tone and register\n",
    "- Ensure accessibility to target audience\n",
    "\n",
    "11. Format Specifications\n",
    "- Use standard paragraph structure\n",
    "- Apply consistent spacing\n",
    "- Implement appropriate punctuation\n",
    "- Maintain professional presentation\n",
    "- Ensure readable layout\n",
    "\n",
    "12. Verification Steps\n",
    "- Review for content accuracy\n",
    "- Check for gender-neutral language\n",
    "- Confirm word count\n",
    "- Validate factual consistency\n",
    "- Ensure logical flow\n",
    "\n",
    "13. Language Precision\n",
    "- Use specific rather than general terms\n",
    "- Maintain technical accuracy\n",
    "- Preserve specialized terminology\n",
    "- Ensure precise word choice\n",
    "- Avoid ambiguity\n",
    "\n",
    "14. Accessibility Guidelines\n",
    "- Use clear language\n",
    "- Maintain reasonable sentence length\n",
    "- Ensure logical organization\n",
    "- Provide clear connections\n",
    "- Use appropriate vocabulary level\n",
    "\n",
    "15. Quality Assurance\n",
    "- Review for completeness\n",
    "- Check for accuracy\n",
    "- Verify neutral language\n",
    "- Confirm proper structure\n",
    "- Validate technical details\n",
    "\n",
    "The summary should serve as a standalone piece that accurately represents the original text while maintaining professional standards and inclusive language. The final product should be clear, concise, and faithful to the source material while being accessible to a general audience.```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f7f21f15-abab-42c4-9889-73d88a5bfd13",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\" Customer: *sighs heavily* Ugh,\n",
    "finally someone picked up. I've been trying to \n",
    "call your customer support line for the past 30 minutes,\n",
    "and it's been ringing non-stop. This is so frustrating! \n",
    "Agent: I apologize for the delay, sir. Thank you for your \n",
    "patience. This is {NAME} from Oktank customer support. \n",
    "How can I assist you today? Customer: *irritated tone* \n",
    "Finally! Okay, look, I signed up for your website a few weeks ago,\n",
    "and I was promised a special promotional offer. But when I went to\n",
    "check out, it wasn't applied to my cart. I've been trying to call\n",
    "all day to figure this out, and no one has been answering. This is\n",
    "unacceptable! Agent: *calmly* I understand your frustration, sir.\n",
    "Let me look into this for you. Could you please provide me with\n",
    "the email address you used to sign up with Oktank? Customer: *sighs*\n",
    "Yeah, it's {EMAIL}. Agent: *typing* Okay, let me pull up your account.\n",
    "*pauses* Ah, I see the issue here. It looks like the promotional\n",
    "offer you were promised was a limited-time deal that expired a\n",
    "few days ago. I apologize for the confusion. Customer: \n",
    "*frustrated* What? No, that can't be right. I signed up\n",
    "just last week, and the offer was supposed to be valid\n",
    "for 30 days. This is ridiculous! Agent: *calmly* Let me \n",
    "double-check that for you, sir. *typing* Hmm, it appears \n",
    "there may have been a technical issue with the website that\n",
    "caused the offer to expire prematurely. Please bear with me \n",
    "for a moment while I look into this further. *the call suddenly\n",
    "becomes laggy, and the agent's voice becomes broken and distorted*\n",
    "Customer: *annoyed* Hello? Hello? Can you hear me? This connection \n",
    "is terrible! What's going on? Agent: *voice breaking up* \n",
    "I... apologize... for... the... technical... difficulties.\n",
    "*static* Let... me... try... to... reconnect... *more static* \n",
    "Customer: *frustrated* Ugh, this is so frustrating! I can barely \n",
    "hear you. Is there someone else I can talk to? This is unacceptable! \n",
    "Agent: *voice clearing up* I'm so sorry about that, sir. It seems\n",
    "we're experiencing some technical difficulties with the phone line.\n",
    "Let me try to reconnect you to ensure we have a clear line of \n",
    "communication. *the call reconnects, and the agent's voice is \n",
    "clear again* Agent: There we go, can you hear me clearly now? \n",
    "Customer: *still irritated* Yes, I can hear you now. What's the\n",
    "deal with this promotional offer? I need to get this resolved.\n",
    "Agent: *calmly* Absolutely, let me look into that for you. *typing*\n",
    "Okay, I've reviewed the details of the promotion you were promised,\n",
    "and it appears there was indeed a technical issue that caused the\n",
    "offer to expire prematurely. That was our error, and I sincerely \n",
    "apologize for the inconvenience. Customer: *frustrated* Well,\n",
    "that's great. So what are you going to do about it? I signed \n",
    "up specifically for that offer, and now I'm being told it's no\n",
    "longer valid. This is unacceptable! Agent: *reassuring tone* I\n",
    "understand your frustration, sir. As a gesture of goodwill,\n",
    "I'd like to offer you an alternative promotion that is currently \n",
    "active on our website. This offer includes free shipping and a \n",
    "15% discount on your entire order. Would that be something you'd \n",
    "be interested in? Customer: *hesitant* I guess that's better \n",
    "than nothing, but it's still not the same as the original\n",
    "offer I was promised. I'm not sure I'm satisfied with that. \n",
    "Agent: *empathetic* I completely understand your disappointment,\n",
    "sir. Let me see if there's anything else I can do to make this\n",
    "right. *typing* Ah, I have an idea. Since the technical issue was\n",
    "on our end, I'd be happy to apply the original promotional offer \n",
    "to your account, even though it has expired. Would that be a suitable\n",
    "resolution? Customer: *surprised* Really? You can do that? *pauses* \n",
    "Okay, that sounds fair. I appreciate you taking the time to look into \n",
    "this and find a solution. Agent: *warmly* Absolutely, sir. I'm happy\n",
    "I could find a way to honor the original offer you were promised.\n",
    "Let me go ahead and apply that to your account now. *typing* There \n",
    "we go, the discount has been applied. You should see the updated \n",
    "total in your cart. Customer: *relieved* Great, let me check. \n",
    "*pauses* Yep, that looks right. Thank you so much for your help.\n",
    "I'm glad we were able to get this resolved. Agent: *sincerely* \n",
    "I'm so glad we could find a solution that works for you, sir. \n",
    "Please don't hesitate to reach out if you have any other questions \n",
    "or concerns. I'm always happy to assist. Customer: *appreciative*\n",
    "Will do. Thanks again for your patience and for taking the time to \n",
    "really look into this. I appreciate it. Agent: *warmly* It's my\n",
    "pleasure, sir. I'm glad I could help. Please let me know if there's\n",
    "anything else I can do for you. Have a wonderful rest of your day! \n",
    "Customer: *politely* You too. Bye. Agent: *cheerfully* Goodbye!\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abf82528-6c72-427c-912f-02f461eb5eff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the 250-word gender-neutral summary:\n",
      "\n",
      "A customer contacted Oktank's customer support after experiencing difficulties with a promotional offer. The customer had signed up for a website promotion but found the offer was not applied during checkout. After multiple attempts to reach support, they finally connected with an agent.\n",
      "\n",
      "The agent investigated the account and discovered a technical issue had caused the promotional offer to expire prematurely. During the initial conversation, the phone connection became unstable, with significant audio disruptions that frustrated the customer.\n",
      "\n",
      "After reconnecting, the agent explained the technical problem and apologized for the inconvenience. The customer expressed continued frustration about the unavailable promotion they had specifically signed up to receive.\n",
      "\n",
      "The agent proposed an alternative promotion featuring free shipping and a 15% discount, which the customer found unsatisfactory. Recognizing the customer's disappointment, the agent then offered to apply the original promotional offer despite its technical expiration as a goodwill gesture.\n",
      "\n",
      "The customer was surprised but appreciated this resolution. The agent processed the original discount, allowing the customer to verify the updated cart total. The interaction concluded positively, with the customer expressing gratitude for the agent's patience and problem-solving approach.\n",
      "\n",
      "The conversation demonstrated the support team's commitment to resolving customer issues by identifying the root cause of the problem, offering multiple solutions, and ultimately finding a mutually acceptable resolution that addressed the customer's initial concerns.\n"
     ]
    }
   ],
   "source": [
    "# Set the model ID, e.g., Claude 3 Haiku.\n",
    "model_id = \"us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "\n",
    "# Define the prompt for the model.\n",
    "\n",
    "# Format the request payload using the model's native structure.\n",
    "native_request = {\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"max_tokens\": 512,\n",
    "    \"temperature\": 0.5,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\", \n",
    "                    \"text\": instruction_template,\n",
    "                    \"cache_control\": {\n",
    "                        \"type\": \"ephemeral\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": prompt\n",
    "                }\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "# Convert the native request to JSON.\n",
    "request = json.dumps(native_request)\n",
    "\n",
    "try:\n",
    "    # Invoke the model with the request.\n",
    "    response = client.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=request\n",
    "    )\n",
    "\n",
    "except (ClientError, Exception) as e:\n",
    "    print(f\"ERROR: Can't invoke '{model_id}'. Reason: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Decode the response body.\n",
    "model_response = json.loads(response[\"body\"].read())\n",
    "\n",
    "# Extract and print the response text.\n",
    "response_text = model_response[\"content\"][0][\"text\"]\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "592919b9-37be-4b26-aa38-087a4bdd73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a dataset for call transcripts\n",
    "# {\"recordId\": \"1739215376\", \"modelInput\": {\"anthropic_version\": \"bedrock-2023-05-31\", \"messages\": [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Write an accurate 250 word gender-neutral summary of the following text without adding preamble or additional information not present in the original text: Hello world\"}]}], \"max_tokens\": 300, \"temperature\": 0.1, \"top_p\": 0.1, \"top_k\": 100}}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b33fd0b-9eed-4e81-8407-c4f7cf7488f6",
   "metadata": {},
   "source": [
    "{\n",
    "    \"recordId\": \"1739215376\",\n",
    "    \"modelInput\": {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": instruction_template ,\n",
    "                        \"cache_control\": {\n",
    "                            \"type\": \"ephemeral\"\n",
    "                        }\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"text\",\n",
    "                        \"text\": subset[i]['document']\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.5,\n",
    "        \"top_k\": 100\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "289b31db-d768-4eaa-b14a-48e85ea00e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c356bc3d81f64991b7e781264ae27d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.24k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e2c8c6541e4dc1bc7454f43c1cac1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "xsum.py:   0%|          | 0.00/5.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ca9b730f384481b9d1e2ffe139fb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/304M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8606b7bb017547ae99587c53d80aec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5bdc14a0f24f5e9609b1b8ed812fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0000.parquet:   0%|          | 0.00/17.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa913ffe4ade4c41913c981c4cb60f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/204045 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94305344d95247d3a581662bb0aadee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/11332 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdaacc33ce04e649f4b810fde41ef3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11334 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"EdinburghNLP/xsum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "900246af-5419-4b07-bae1-e8338eb7108c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 204045\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11332\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['document', 'summary', 'id'],\n",
       "        num_rows: 11334\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "40151288-46ac-4ded-8847-ee8287b79235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random 30,000 rows from the training set\n",
    "subset = ds['train'].shuffle(seed=42).select(range(30000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9cac9ae7-a2e6-4a83-8d87-132ddb805493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['document', 'summary', 'id'],\n",
       "    num_rows: 30000\n",
       "})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6627b4fb-24c2-41bc-b085-ef61ea508c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Wales, councils are responsible for funding and overseeing schools.\n",
      "But in England, Mr Osborne's plan will mean local authorities will cease to have a role in providing education.\n",
      "Academies are directly funded by central government and head teachers have more freedom over admissions and to change the way the school works.\n",
      "It is a significant development in the continued divergence of schools systems on either side of Offa's Dyke.\n",
      "And although the Welsh Government will get extra cash to match the money for English schools to extend the school day, it can spend it on any devolved policy area.\n",
      "Ministers have no plans to follow suit.\n",
      "At the moment, governing bodies are responsible for setting school hours and they need ministerial permission to make significant changes.\n",
      "There are already more than 2,000 secondary academies in England and its extension to all state schools is unlikely to shake the Welsh Government's attachment to what they call a \"community, comprehensive model\" for schools.\n",
      "It rejects claims that freedom given to academies can help drive up standards, and it points to academy-free Scotland as the best performing school system in the UK.\n",
      "Education Minister Huw Lewis said there was \"very little evidence to suggest\" academies have a positive impact in driving up standards and Wales would not be following the model.\n",
      "\"The Tories have wasted hundreds of millions of pounds on academies and free schools and as the Chancellor finalises his budget plans to slash vital services even further, he is committing them to wasting even more on a failing endeavour.\n",
      "\"We have no plans to introduce the chaos and waste of academies and free schools here in Wales.\"\n",
      "None of the main parties in May's Assembly election - including the Welsh Conservatives - have said they want to introduce academies in Wales.\n",
      "Owen Hathway, NUT Cymru's policy officer, called the academy plans for England \"scandalous.\".\n",
      "\"There is no evidence that academies work, no evidence that they raise standards, no evidence that they offer better quality education and no evidence that they are what parents and communities want,\" he said.\n",
      "\"Certainly a commitment to comprehensive education is something we would want, and indeed expect, all parties to hold firm to in their manifestos for the forthcoming Welsh election.\"\n",
      "But the Welsh and English schools systems are still linked by a joint arrangement for teachers' pay and conditions.\n",
      "Academies are not tied to these pay scales so in effect Wednesday's announcement will take all English schools out of the system and raise questions about the viability of an England and Wales pay and conditions structure.\n",
      "There is already growing momentum for the devolution of teachers' pay and conditions.\n",
      "Originally sceptical, the Welsh Labour Government is now broadly in favour.\n",
      "Some teaching unions remain opposed because of concern that Welsh teachers would end up being paid less than those in England.\n",
      "Mr Hathway said teachers were concerned it could lead to regional pay.\n",
      "\"At the same time we do of course recognise that the issue of pay is already becoming a grey area due to the negative changes we see taking place in England,\" he said.\n",
      "But an even bigger difference between the schools landscape on either side of the border, appears to make separate arrangements for pay increasingly likely in future.\n"
     ]
    }
   ],
   "source": [
    "print(subset[0]['document'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60a79038-c54e-4cbb-bc91-ff3e1ab8ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to generate a random 10-digit number\n",
    "id_counter = 1000000000  \n",
    "\n",
    "# Open file in write mode\n",
    "with open('batchinferenceHaikuthreepointfivecache.jsonl', 'w', encoding='utf-8') as f:\n",
    "    # Iterate through the dataset\n",
    "    for item in subset:\n",
    "        # Create the JSON structure\n",
    "        record = {\n",
    "            \"recordId\": str(id_counter),\n",
    "            \"modelInput\": {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"messages\": [\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": instruction_template,\n",
    "                                \"cache_control\": {\n",
    "                                    \"type\": \"ephemeral\"\n",
    "                                }\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": item['document']\n",
    "                            }\n",
    "                        ]\n",
    "                    }\n",
    "                ],\n",
    "                \"max_tokens\": 512,\n",
    "                \"temperature\": 0.5,\n",
    "                \"top_k\": 100\n",
    "            }\n",
    "        }\n",
    "        id_counter += 1 \n",
    "        \n",
    "        # Write the JSON object as a line in the file\n",
    "        f.write(json.dumps(record) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e263e852-8e83-4970-a771-75781ed46b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonlfile = \"batchinferenceHaikuthreepointfivecache.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ebedb8eb-5f21-48e7-8f28-f95f92296553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully uploaded file to s3://virgilio-murillo/virgilio-murillo/input/batchinferenceHaikuthreepointfivecache.jsonl\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Define bucket and path\n",
    "bucket_name = 'virgilio-murillo'\n",
    "s3_path = 'virgilio-murillo/input/batchinferenceHaikuthreepointfivecache.jsonl'\n",
    "\n",
    "# Upload file\n",
    "try:\n",
    "    s3_client.upload_file(\n",
    "        'batchinferenceHaikuthreepointfivecache.jsonl',  # Local file name\n",
    "        bucket_name,  # Bucket name\n",
    "        s3_path  # S3 path\n",
    "    )\n",
    "    print(f\"Successfully uploaded file to s3://{bucket_name}/{s3_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84a057e0-2fdb-4d75-ac9b-1fdb302a9d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "32a0dc16-2b5d-4f43-b716-60c01e3020c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:sts::794038231401:assumed-role/SageMaker-ExecutionRole-20250401T103257/SageMaker\n"
     ]
    }
   ],
   "source": [
    "# first we will find out which user or role is being used by sagemaker\n",
    "# We will assign BedrockFUllAccess to it. \n",
    "session = boto3.Session()\n",
    "sts_client = session.client('sts')\n",
    "identity = sts_client.get_caller_identity()\n",
    "print(identity['Arn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21cd5d46-0d0c-4dd7-aeef-c47da296f569",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock = boto3.client(service_name='bedrock', region_name=\"us-east-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d86270d0-bdd7-4b20-8f49-e7200f9783da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:bedrock:us-east-1:794038231401:model-invocation-job/44ljvht2kv9x\n"
     ]
    }
   ],
   "source": [
    "# Option 1: Using Unix timestamp\n",
    "timestamp = int(time.time())\n",
    "jobname = f\"summaries{timestamp}\"\n",
    "\n",
    "\n",
    "inputDataConfig=({\n",
    "    \"s3InputDataConfig\": {\n",
    "        \"s3Uri\": \"s3://virgilio-murillo/virgilio-murillo/input/batchinferenceHaikuthreepointfivecache.jsonl\"\n",
    "    }\n",
    "})\n",
    "\n",
    "outputDataConfig=({\n",
    "    \"s3OutputDataConfig\": {\n",
    "        \"s3Uri\": \"s3://virgilio-murillo/virgilio-murillo/output/\"\n",
    "    }\n",
    "})\n",
    "\n",
    "\n",
    "response = bedrock.create_model_invocation_job(\n",
    "    jobName=jobname,\n",
    "    modelId = model_id,\n",
    "    roleArn = \"arn:aws:iam::794038231401:role/service-role/AmazonSageMaker-ExecutionRole-20241108T131283\",\n",
    "    inputDataConfig = inputDataConfig,\n",
    "    outputDataConfig = outputDataConfig\n",
    ")\n",
    "identity['Arn']\n",
    "\n",
    "jobArn = response.get('jobArn')\n",
    "print(jobArn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a1e808-2c37-4fa0-aa11-2dce9ea0234f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
