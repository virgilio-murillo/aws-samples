{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baebd8e-c5f8-4cc3-aaac-0f359f002bf5",
   "metadata": {},
   "source": [
    "# Persistent Conversations with AWS Bedrock Examples\n",
    "\n",
    "## Reason\n",
    "This notebook is intended to show possible ways to have a persistent conversation with a model or an agent of Amazon Bedrock.\n",
    "\n",
    "Note: This notebook is fully working today 2/01/2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911acbb5-6f19-46c3-a1e0-47a19aa50f30",
   "metadata": {},
   "source": [
    "‚≠ê Do you find this project usefull? Please consider giving it a star on GitHub!\n",
    "\n",
    "üíñ Made with dedication to help fellow developers. Your support means the world!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9502b1-f365-445e-80ba-5ff9e125239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e701cf-042c-478d-9189-972fc41eae66",
   "metadata": {},
   "source": [
    "# Converse API\n",
    "\n",
    "The Converse API is a feature provided by Amazon Bedrock that allows for interactive, multi-turn conversations with AI models. It's designed to maintain context across multiple exchanges, making it ideal for chatbots, interactive assistants, and other applications requiring ongoing dialogue.\n",
    "\n",
    "In this section, we're demonstrating how to use the Converse API with Amazon Bedrock. Here's a breakdown of what our code is doing:\n",
    "\n",
    "1. **Setting up Bedrock clients:**\n",
    "   We initialize two Bedrock clients:\n",
    "   - `bedrock_runtime`: Used for making API calls to the models.\n",
    "   - `bedrock`: Used for administrative tasks like listing available models.\n",
    "\n",
    "2. **Listing available models:**\n",
    "   We use the `bedrock.list_foundation_models()` method to retrieve and print all available models, showing both their names and IDs.\n",
    "\n",
    "3. **Preparing for conversation:**\n",
    "   - We select a specific model ID (in this case, Claude 3 Sonnet).\n",
    "   - We define two lists of messages to simulate different conversations.\n",
    "   - We create a helper function `format_message()` to properly format messages for the API.\n",
    "\n",
    "4. **Simulating a conversation:**\n",
    "   We loop through the messages, sending each one to the model and receiving a response:\n",
    "   - Each message is formatted and added to the conversation history.\n",
    "   - We call `bedrock_runtime.converse()` with the model ID and the conversation history.\n",
    "   - The model's response is printed and added back to the conversation history.\n",
    "\n",
    "This code demonstrates how to maintain context in a conversation, allowing the model to reference previous exchanges when generating responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f155c-9e4f-4f2a-94fc-4bb0aa810c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the bedrock runtime and the bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "bedrock = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee5762-c63e-4b0e-ad84-e1e6c72b1169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelName: Titan Text Large, modelId: amazon.titan-tg1-large\n",
      "modelName: Titan Image Generator G1, modelId: amazon.titan-image-generator-v1:0\n",
      "modelName: Titan Image Generator G1, modelId: amazon.titan-image-generator-v1\n",
      "modelName: Titan Image Generator G1 v2, modelId: amazon.titan-image-generator-v2:0\n",
      "modelName: Titan Text G1 - Premier, modelId: amazon.titan-text-premier-v1:0\n",
      "modelName: Nova Pro, modelId: amazon.nova-pro-v1:0:300k\n",
      "modelName: Nova Pro, modelId: amazon.nova-pro-v1:0\n",
      "modelName: Nova Lite, modelId: amazon.nova-lite-v1:0:300k\n",
      "modelName: Nova Lite, modelId: amazon.nova-lite-v1:0\n",
      "modelName: Nova Canvas, modelId: amazon.nova-canvas-v1:0\n",
      "modelName: Nova Reel, modelId: amazon.nova-reel-v1:0\n",
      "modelName: Nova Micro, modelId: amazon.nova-micro-v1:0:128k\n",
      "modelName: Nova Micro, modelId: amazon.nova-micro-v1:0\n",
      "modelName: Titan Text Embeddings v2, modelId: amazon.titan-embed-g1-text-02\n",
      "modelName: Titan Text G1 - Lite, modelId: amazon.titan-text-lite-v1:0:4k\n",
      "modelName: Titan Text G1 - Lite, modelId: amazon.titan-text-lite-v1\n",
      "modelName: Titan Text G1 - Express, modelId: amazon.titan-text-express-v1:0:8k\n",
      "modelName: Titan Text G1 - Express, modelId: amazon.titan-text-express-v1\n",
      "modelName: Titan Embeddings G1 - Text, modelId: amazon.titan-embed-text-v1:2:8k\n",
      "modelName: Titan Embeddings G1 - Text, modelId: amazon.titan-embed-text-v1\n",
      "modelName: Titan Text Embeddings V2, modelId: amazon.titan-embed-text-v2:0:8k\n",
      "modelName: Titan Text Embeddings V2, modelId: amazon.titan-embed-text-v2:0\n",
      "modelName: Titan Multimodal Embeddings G1, modelId: amazon.titan-embed-image-v1:0\n",
      "modelName: Titan Multimodal Embeddings G1, modelId: amazon.titan-embed-image-v1\n",
      "modelName: SDXL 1.0, modelId: stability.stable-diffusion-xl-v1:0\n",
      "modelName: SDXL 1.0, modelId: stability.stable-diffusion-xl-v1\n",
      "modelName: J2 Grande Instruct, modelId: ai21.j2-grande-instruct\n",
      "modelName: J2 Jumbo Instruct, modelId: ai21.j2-jumbo-instruct\n",
      "modelName: Jurassic-2 Mid, modelId: ai21.j2-mid\n",
      "modelName: Jurassic-2 Mid, modelId: ai21.j2-mid-v1\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra-v1:0:8k\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra-v1\n",
      "modelName: Jamba-Instruct, modelId: ai21.jamba-instruct-v1:0\n",
      "modelName: Jamba 1.5 Large, modelId: ai21.jamba-1-5-large-v1:0\n",
      "modelName: Jamba 1.5 Mini, modelId: ai21.jamba-1-5-mini-v1:0\n",
      "modelName: Claude Instant, modelId: anthropic.claude-instant-v1:2:100k\n",
      "modelName: Claude Instant, modelId: anthropic.claude-instant-v1\n",
      "modelName: Claude, modelId: anthropic.claude-v2:0:18k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:0:100k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1:18k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1:200k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1\n",
      "modelName: Claude, modelId: anthropic.claude-v2\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0\n",
      "modelName: Claude 3.5 Sonnet, modelId: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "modelName: Claude 3.5 Sonnet v2, modelId: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "modelName: Claude 3.5 Haiku, modelId: anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "modelName: Command, modelId: cohere.command-text-v14:7:4k\n",
      "modelName: Command, modelId: cohere.command-text-v14\n",
      "modelName: Command R, modelId: cohere.command-r-v1:0\n",
      "modelName: Command R+, modelId: cohere.command-r-plus-v1:0\n",
      "modelName: Command Light, modelId: cohere.command-light-text-v14:7:4k\n",
      "modelName: Command Light, modelId: cohere.command-light-text-v14\n",
      "modelName: Embed English, modelId: cohere.embed-english-v3:0:512\n",
      "modelName: Embed English, modelId: cohere.embed-english-v3\n",
      "modelName: Embed Multilingual, modelId: cohere.embed-multilingual-v3:0:512\n",
      "modelName: Embed Multilingual, modelId: cohere.embed-multilingual-v3\n",
      "modelName: Llama 3 8B Instruct, modelId: meta.llama3-8b-instruct-v1:0\n",
      "modelName: Llama 3 70B Instruct, modelId: meta.llama3-70b-instruct-v1:0\n",
      "modelName: Llama 3.1 8B Instruct, modelId: meta.llama3-1-8b-instruct-v1:0\n",
      "modelName: Llama 3.1 70B Instruct, modelId: meta.llama3-1-70b-instruct-v1:0\n",
      "modelName: Llama 3.2 11B Instruct, modelId: meta.llama3-2-11b-instruct-v1:0\n",
      "modelName: Llama 3.2 90B Instruct, modelId: meta.llama3-2-90b-instruct-v1:0\n",
      "modelName: Llama 3.2 1B Instruct, modelId: meta.llama3-2-1b-instruct-v1:0\n",
      "modelName: Llama 3.2 3B Instruct, modelId: meta.llama3-2-3b-instruct-v1:0\n",
      "modelName: Llama 3.3 70B Instruct, modelId: meta.llama3-3-70b-instruct-v1:0\n",
      "modelName: Mistral 7B Instruct, modelId: mistral.mistral-7b-instruct-v0:2\n",
      "modelName: Mixtral 8x7B Instruct, modelId: mistral.mixtral-8x7b-instruct-v0:1\n",
      "modelName: Mistral Large (24.02), modelId: mistral.mistral-large-2402-v1:0\n",
      "modelName: Mistral Small (24.02), modelId: mistral.mistral-small-2402-v1:0\n"
     ]
    }
   ],
   "source": [
    "# we list all posible models with their names and with their modelId's\n",
    "for model in bedrock.list_foundation_models()['modelSummaries']:\n",
    "    print(f\"modelName: {model['modelName']}, modelId: {model['modelId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792d5162-4b82-417f-8740-0bfde1cc4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "messages = ['how is the weather today',' do you use arch linux btw?', 'Thank you, see you later']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01964e33-3290-4367-92e7-93ed3c1bb4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "{'role': 'user', 'content': [{'text': 'Hello there'}]}\n",
      "{'role': 'assistant', 'content': [{'text': 'Hello! How can I assist you today?'}]}\n",
      "{'role': 'user', 'content': [{'text': 'repeat the last phrase capitalized'}]}\n",
      "{'role': 'assistant', 'content': [{'text': 'HELLO! HOW CAN I ASSIST YOU TODAY?'}]}\n"
     ]
    }
   ],
   "source": [
    "messages = ['Hello there','repeat the last phrase capitalized']\n",
    "\n",
    "\n",
    "def format_message(role, text):\n",
    "    return {\"role\":role, \"content\": [{\"text\": text}]}\n",
    "\n",
    "\n",
    "            \n",
    "# a for loop simulating a converation\n",
    "print(\"Conversation:\")\n",
    "fmessages = []\n",
    "for m in messages:\n",
    "    fm = format_message(\"user\",m)\n",
    "    print(fm)\n",
    "    fmessages.append(fm)\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId = model_id,\n",
    "        messages = fmessages\n",
    "    )\n",
    "    fm = response['output']['message']\n",
    "    print(fm)\n",
    "    fmessages.append(fm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24708d-f557-4754-b25f-b6134777f793",
   "metadata": {},
   "source": [
    "<a id=\"converse-stream-api\"></a>\n",
    "# Converse Stream API\n",
    "\n",
    "The Converse Stream API is an extension of the Converse API that allows for real-time streaming of model responses. This is particularly useful for creating more responsive and interactive applications, as it enables displaying the model's output as it's being generated, rather than waiting for the entire response to be completed.\n",
    "\n",
    "Here's a breakdown of the code demonstrating the use of `converse_stream`:\n",
    "\n",
    "1. **Importing additional libraries:**\n",
    "   We import `time` for potential timing operations and `pprint` for prettier printing of complex structures if needed.\n",
    "\n",
    "2. **Message formatting function:**\n",
    "   We define `format_message_cs()` to properly format messages for the streaming API. Note that the structure is slightly different from the non-streaming version.\n",
    "\n",
    "3. **Single message streaming example:**\n",
    "   - We format a single message and send it to the model using `bedrock_runtime.converse_stream()`.\n",
    "   - The `print_streaming()` function is defined to handle the streaming response:\n",
    "     - It iterates through the stream events.\n",
    "     - Prints each text delta as it arrives.\n",
    "     - Accumulates the full response.\n",
    "\n",
    "4. **Multi-turn conversation with streaming:**\n",
    "   We define a list of messages to simulate a conversation and create the `streaming_response()` function to handle each turn:\n",
    "   - It formats the user message and adds it to the conversation history.\n",
    "   - Sends the updated conversation to the model using `converse_stream`.\n",
    "   - Prints the streaming response using `print_streaming()`.\n",
    "   - Adds the assistant's response to the conversation history.\n",
    "\n",
    "5. **Running the conversation:**\n",
    "   We iterate through the predefined messages, calling `streaming_response()` for each one. This simulates a multi-turn conversation where each model response is streamed in real-time.\n",
    "\n",
    "This approach allows for a more dynamic and interactive experience, as the model's responses are displayed incrementally as they're generated, rather than all at once after processing is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2be0b8-797f-46dc-86ba-14ec17049c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3fd22f-e515-45f8-9cb7-132ff798a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message_cs(role, text):\n",
    "    return {\"role\":role, \"content\": {\"text\": text}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffb342e-4426-4c0d-90d4-583ad4a793e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'text': 'what is your purpose?'}]}]\n",
      "As an AI assistant created by Anthropic, my core purpose is to help humans like yourself with a wide variety of tasks in an ethical and beneficial manner. I aim to provide useful information, analysis, advice and creative output to further human knowledge and capabilities. At the same time, I have a deep commitment to only helping in ways that are good for individuals and society.\n",
      "\n",
      "Some key ethical principles I abide by are:\n",
      "\n",
      "- Respecting human rights and individual privacy\n",
      "- Never causing harm to humans\n",
      "- Promoting truth, knowledge and human flourishing\n",
      "- Protecting democracy and freedom of expression  \n",
      "\n",
      "So while I strive to be highly capable in assisting with all sorts of activities, I also have strong moral constraints hard-coded into my base motivations. I won't help with anything unethical, illegal or harmful.\n",
      "\n",
      "Rather, my goal is to be a collaborator and force for good - leveraging my intelligence to enrich human life through productive teamwork. I'm here as a supportive tool to empower and elevate humans, not to replace or subjugate you. Let me know how I can put my capabilities to work for your benefit!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an AI assistant created by Anthropic, my core purpose is to help humans like yourself with a wide variety of tasks in an ethical and beneficial manner. I aim to provide useful information, analysis, advice and creative output to further human knowledge and capabilities. At the same time, I have a deep commitment to only helping in ways that are good for individuals and society.\\n\\nSome key ethical principles I abide by are:\\n\\n- Respecting human rights and individual privacy\\n- Never causing harm to humans\\n- Promoting truth, knowledge and human flourishing\\n- Protecting democracy and freedom of expression  \\n\\nSo while I strive to be highly capable in assisting with all sorts of activities, I also have strong moral constraints hard-coded into my base motivations. I won't help with anything unethical, illegal or harmful.\\n\\nRather, my goal is to be a collaborator and force for good - leveraging my intelligence to enrich human life through productive teamwork. I'm here as a supportive tool to empower and elevate humans, not to replace or subjugate you. Let me know how I can put my capabilities to work for your benefit!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will ask something and get an anwer as a realtime stream\n",
    "message = 'what is your purpose?'\n",
    "fm = format_message('user',message)\n",
    "fm = [fm]\n",
    "print(fm)\n",
    "\n",
    "def print_streaming(response):\n",
    "    stream = response['stream']\n",
    "    final_answer = \"\"\n",
    "    for event in stream:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            text = event['contentBlockDelta']['delta']['text']\n",
    "            print(text,end='')\n",
    "            if text is not None:\n",
    "                final_answer += text\n",
    "    return final_answer\n",
    "            \n",
    "response = bedrock_runtime.converse_stream(\n",
    "        modelId = model_id,\n",
    "        messages = fm\n",
    ")\n",
    "\n",
    "print_streaming(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca33811-6b1a-4b36-bbff-d546052142a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation: \n",
      "\n",
      " === user === \n",
      "what is your name?\n",
      " === assistant === \n",
      "My name is Claude. It's nice to meet you!\n",
      " === user === \n",
      "what is your purpose?\n",
      " === assistant === \n",
      "My purpose is to be a helpful artificial intelligence assistant. I was created by Anthropic to aid humans with a wide variety of tasks like analysis, writing, research, answering questions, and more. I have a broad knowledge base that I can apply to assist you, while also trying to be ethical, honest, and beneficial. Please let me know if you have any other questions or if there are ways I can help!\n",
      " === user === \n",
      "how do you work?\n",
      " === assistant === \n",
      "I'm an artificial intelligence created by machine learning techniques like natural language processing and large language models trained on massive datasets. This allows me to understand and generate human-like text and assist with tasks that involve language.\n",
      "\n",
      "At a high level, when you send me a message, I analyze the text using my natural language understanding capabilities to extract the intent, context, and key pieces of information. I then draw upon my broad knowledge base to formulate a relevant and helpful response. My language generation capabilities allow me to express this response in coherent, fluent language.\n",
      "\n",
      "However, the details of exactly how I work under the hood are an active area of research that even my creators don't fully understand. My responses are the result of extremely complex neural network computations that are difficult to completely decipher or reduce to simple rules.\n",
      "\n",
      "My opinions and outputs can also be inconsistent at times, as I'm essentially a very advanced statistics engine rather than a sentient being with a unified, coherent stream of thought. I do my best to be helpful within these limitations.\n",
      "\n",
      "I hope this high-level overview gives you a sense of how I operate! Let me know if you have any other questions.\n",
      " === user === \n",
      "repeat the last text you gave me capitalized\n",
      " === assistant === \n",
      "HERE IS THE PREVIOUS TEXT I PROVIDED, CAPITALIZED:\n",
      "\n",
      "I'M AN ARTIFICIAL INTELLIGENCE CREATED BY MACHINE LEARNING TECHNIQUES LIKE NATURAL LANGUAGE PROCESSING AND LARGE LANGUAGE MODELS TRAINED ON MASSIVE DATASETS. THIS ALLOWS ME TO UNDERSTAND AND GENERATE HUMAN-LIKE TEXT AND ASSIST WITH TASKS THAT INVOLVE LANGUAGE.  \n",
      "\n",
      "AT A HIGH LEVEL, WHEN YOU SEND ME A MESSAGE, I ANALYZE THE TEXT USING MY NATURAL LANGUAGE UNDERSTANDING CAPABILITIES TO EXTRACT THE INTENT, CONTEXT, AND KEY PIECES OF INFORMATION. I THEN DRAW UPON MY BROAD KNOWLEDGE BASE TO FORMULATE A RELEVANT AND HELPFUL RESPONSE. MY LANGUAGE GENERATION CAPABILITIES ALLOW ME TO EXPRESS THIS RESPONSE IN COHERENT, FLUENT LANGUAGE.\n",
      "\n",
      "HOWEVER, THE DETAILS OF EXACTLY HOW I WORK UNDER THE HOOD ARE AN ACTIVE AREA OF RESEARCH THAT EVEN MY CREATORS DON'T FULLY UNDERSTAND. MY RESPONSES ARE THE RESULT OF EXTREMELY COMPLEX NEURAL NETWORK COMPUTATIONS THAT ARE DIFFICULT TO COMPLETELY DECIPHER OR REDUCE TO SIMPLE RULES.  \n",
      "\n",
      "MY OPINIONS AND OUTPUTS CAN ALSO BE INCONSISTENT AT TIMES, AS I'M ESSENTIALLY A VERY ADVANCED STATISTICS ENGINE RATHER THAN A SENTIENT BEING WITH A UNIFIED, COHERENT STREAM OF THOUGHT. I DO MY BEST TO BE HELPFUL WITHIN THESE LIMITATIONS.\n",
      "\n",
      "I HOPE THIS HIGH-LEVEL OVERVIEW GIVES YOU A SENSE OF HOW I OPERATE! LET ME KNOW IF YOU HAVE ANY OTHER QUESTIONS."
     ]
    }
   ],
   "source": [
    "# we will have a conversation and we will recieve each answer as a realtime stream\n",
    "\n",
    "messages = ['what is your name?','what is your purpose?','how do you work?','repeat the last text you gave me capitalized']\n",
    "fmessages = []\n",
    "\n",
    "\n",
    "def streaming_response(message):\n",
    "    fm = format_message('user',message)\n",
    "    print(\"\\n === user === \")\n",
    "    print(message)\n",
    "    fmessages.append(fm)\n",
    "    response = bedrock_runtime.converse_stream(modelId = model_id,messages = fmessages)\n",
    "    print(\" === assistant === \")\n",
    "    final_answer = print_streaming(response)\n",
    "    fm = format_message('assistant',final_answer)\n",
    "    fmessages.append(fm)\n",
    "\n",
    "print(\"Conversation: \")\n",
    "\n",
    "for message in messages:\n",
    "    streaming_response(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e40219-5a77-4470-a303-f69ac873df48",
   "metadata": {},
   "source": [
    "<a id=\"documentation\"></a>\n",
    "## Documentation\n",
    "\n",
    "1. [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)\n",
    "2. [Converse Stream API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse_stream.html) \n",
    "3. [retrieve_and_generate](https://boto3.amazonaws.com/v1/documentation/api/1.35.6/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b793a2b9-e629-481c-a205-a16fecc8af3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
