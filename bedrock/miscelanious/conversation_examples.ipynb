{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8baebd8e-c5f8-4cc3-aaac-0f359f002bf5",
   "metadata": {},
   "source": [
    "# Persistent Conversations with AWS Bedrock Examples\n",
    "\n",
    "## Reason\n",
    "This notebook is intended to show all the possible ways to have a persistent conversation with a model or an agent of Amazon Bedrock.\n",
    "\n",
    "## Sections\n",
    "- [Converse API](#converse-api)\n",
    "- [Converse Stream API](#converse-stream-api)\n",
    "- [Documentation](#documentation)\n",
    "\n",
    "Note: This notebook is fully working today 2/01/2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9502b1-f365-445e-80ba-5ff9e125239e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import json\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e701cf-042c-478d-9189-972fc41eae66",
   "metadata": {},
   "source": [
    "<a id=\"converse-api\"></a>\n",
    "## Converse API\n",
    "\n",
    "Sends messages to the specified Amazon Bedrock model. Converse provides a consistent interface that works with all models that support messages. This allows you to write code once and use it with different models. (refer to [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f155c-9e4f-4f2a-94fc-4bb0aa810c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we get the bedrock runtime and the bedrock client\n",
    "bedrock_runtime = boto3.client('bedrock-runtime')\n",
    "bedrock = boto3.client('bedrock')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aee5762-c63e-4b0e-ad84-e1e6c72b1169",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelName: Titan Text Large, modelId: amazon.titan-tg1-large\n",
      "modelName: Titan Image Generator G1, modelId: amazon.titan-image-generator-v1:0\n",
      "modelName: Titan Image Generator G1, modelId: amazon.titan-image-generator-v1\n",
      "modelName: Titan Image Generator G1 v2, modelId: amazon.titan-image-generator-v2:0\n",
      "modelName: Titan Text G1 - Premier, modelId: amazon.titan-text-premier-v1:0\n",
      "modelName: Nova Pro, modelId: amazon.nova-pro-v1:0:300k\n",
      "modelName: Nova Pro, modelId: amazon.nova-pro-v1:0\n",
      "modelName: Nova Lite, modelId: amazon.nova-lite-v1:0:300k\n",
      "modelName: Nova Lite, modelId: amazon.nova-lite-v1:0\n",
      "modelName: Nova Canvas, modelId: amazon.nova-canvas-v1:0\n",
      "modelName: Nova Reel, modelId: amazon.nova-reel-v1:0\n",
      "modelName: Nova Micro, modelId: amazon.nova-micro-v1:0:128k\n",
      "modelName: Nova Micro, modelId: amazon.nova-micro-v1:0\n",
      "modelName: Titan Text Embeddings v2, modelId: amazon.titan-embed-g1-text-02\n",
      "modelName: Titan Text G1 - Lite, modelId: amazon.titan-text-lite-v1:0:4k\n",
      "modelName: Titan Text G1 - Lite, modelId: amazon.titan-text-lite-v1\n",
      "modelName: Titan Text G1 - Express, modelId: amazon.titan-text-express-v1:0:8k\n",
      "modelName: Titan Text G1 - Express, modelId: amazon.titan-text-express-v1\n",
      "modelName: Titan Embeddings G1 - Text, modelId: amazon.titan-embed-text-v1:2:8k\n",
      "modelName: Titan Embeddings G1 - Text, modelId: amazon.titan-embed-text-v1\n",
      "modelName: Titan Text Embeddings V2, modelId: amazon.titan-embed-text-v2:0:8k\n",
      "modelName: Titan Text Embeddings V2, modelId: amazon.titan-embed-text-v2:0\n",
      "modelName: Titan Multimodal Embeddings G1, modelId: amazon.titan-embed-image-v1:0\n",
      "modelName: Titan Multimodal Embeddings G1, modelId: amazon.titan-embed-image-v1\n",
      "modelName: SDXL 1.0, modelId: stability.stable-diffusion-xl-v1:0\n",
      "modelName: SDXL 1.0, modelId: stability.stable-diffusion-xl-v1\n",
      "modelName: J2 Grande Instruct, modelId: ai21.j2-grande-instruct\n",
      "modelName: J2 Jumbo Instruct, modelId: ai21.j2-jumbo-instruct\n",
      "modelName: Jurassic-2 Mid, modelId: ai21.j2-mid\n",
      "modelName: Jurassic-2 Mid, modelId: ai21.j2-mid-v1\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra-v1:0:8k\n",
      "modelName: Jurassic-2 Ultra, modelId: ai21.j2-ultra-v1\n",
      "modelName: Jamba-Instruct, modelId: ai21.jamba-instruct-v1:0\n",
      "modelName: Jamba 1.5 Large, modelId: ai21.jamba-1-5-large-v1:0\n",
      "modelName: Jamba 1.5 Mini, modelId: ai21.jamba-1-5-mini-v1:0\n",
      "modelName: Claude Instant, modelId: anthropic.claude-instant-v1:2:100k\n",
      "modelName: Claude Instant, modelId: anthropic.claude-instant-v1\n",
      "modelName: Claude, modelId: anthropic.claude-v2:0:18k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:0:100k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1:18k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1:200k\n",
      "modelName: Claude, modelId: anthropic.claude-v2:1\n",
      "modelName: Claude, modelId: anthropic.claude-v2\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
      "modelName: Claude 3 Sonnet, modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0:48k\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0:200k\n",
      "modelName: Claude 3 Haiku, modelId: anthropic.claude-3-haiku-20240307-v1:0\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:12k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:28k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0:200k\n",
      "modelName: Claude 3 Opus, modelId: anthropic.claude-3-opus-20240229-v1:0\n",
      "modelName: Claude 3.5 Sonnet, modelId: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "modelName: Claude 3.5 Sonnet v2, modelId: anthropic.claude-3-5-sonnet-20241022-v2:0\n",
      "modelName: Claude 3.5 Haiku, modelId: anthropic.claude-3-5-haiku-20241022-v1:0\n",
      "modelName: Command, modelId: cohere.command-text-v14:7:4k\n",
      "modelName: Command, modelId: cohere.command-text-v14\n",
      "modelName: Command R, modelId: cohere.command-r-v1:0\n",
      "modelName: Command R+, modelId: cohere.command-r-plus-v1:0\n",
      "modelName: Command Light, modelId: cohere.command-light-text-v14:7:4k\n",
      "modelName: Command Light, modelId: cohere.command-light-text-v14\n",
      "modelName: Embed English, modelId: cohere.embed-english-v3:0:512\n",
      "modelName: Embed English, modelId: cohere.embed-english-v3\n",
      "modelName: Embed Multilingual, modelId: cohere.embed-multilingual-v3:0:512\n",
      "modelName: Embed Multilingual, modelId: cohere.embed-multilingual-v3\n",
      "modelName: Llama 3 8B Instruct, modelId: meta.llama3-8b-instruct-v1:0\n",
      "modelName: Llama 3 70B Instruct, modelId: meta.llama3-70b-instruct-v1:0\n",
      "modelName: Llama 3.1 8B Instruct, modelId: meta.llama3-1-8b-instruct-v1:0\n",
      "modelName: Llama 3.1 70B Instruct, modelId: meta.llama3-1-70b-instruct-v1:0\n",
      "modelName: Llama 3.2 11B Instruct, modelId: meta.llama3-2-11b-instruct-v1:0\n",
      "modelName: Llama 3.2 90B Instruct, modelId: meta.llama3-2-90b-instruct-v1:0\n",
      "modelName: Llama 3.2 1B Instruct, modelId: meta.llama3-2-1b-instruct-v1:0\n",
      "modelName: Llama 3.2 3B Instruct, modelId: meta.llama3-2-3b-instruct-v1:0\n",
      "modelName: Llama 3.3 70B Instruct, modelId: meta.llama3-3-70b-instruct-v1:0\n",
      "modelName: Mistral 7B Instruct, modelId: mistral.mistral-7b-instruct-v0:2\n",
      "modelName: Mixtral 8x7B Instruct, modelId: mistral.mixtral-8x7b-instruct-v0:1\n",
      "modelName: Mistral Large (24.02), modelId: mistral.mistral-large-2402-v1:0\n",
      "modelName: Mistral Small (24.02), modelId: mistral.mistral-small-2402-v1:0\n"
     ]
    }
   ],
   "source": [
    "# we list all posible models with their names and with their modelId's\n",
    "for model in bedrock.list_foundation_models()['modelSummaries']:\n",
    "    print(f\"modelName: {model['modelName']}, modelId: {model['modelId']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "792d5162-4b82-417f-8740-0bfde1cc4572",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "messages = ['how is the weather today',' do you use arch linux btw?', 'Thank you, see you later']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01964e33-3290-4367-92e7-93ed3c1bb4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation:\n",
      "{'role': 'user', 'content': [{'text': 'Hello there'}]}\n",
      "{'role': 'assistant', 'content': [{'text': 'Hello! How can I assist you today?'}]}\n",
      "{'role': 'user', 'content': [{'text': 'repeat the last phrase capitalized'}]}\n",
      "{'role': 'assistant', 'content': [{'text': 'HELLO! HOW CAN I ASSIST YOU TODAY?'}]}\n"
     ]
    }
   ],
   "source": [
    "messages = ['Hello there','repeat the last phrase capitalized']\n",
    "\n",
    "\n",
    "def format_message(role, text):\n",
    "    return {\"role\":role, \"content\": [{\"text\": text}]}\n",
    "\n",
    "\n",
    "            \n",
    "# a for loop simulating a converation\n",
    "print(\"Conversation:\")\n",
    "fmessages = []\n",
    "for m in messages:\n",
    "    fm = format_message(\"user\",m)\n",
    "    print(fm)\n",
    "    fmessages.append(fm)\n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId = model_id,\n",
    "        messages = fmessages\n",
    "    )\n",
    "    fm = response['output']['message']\n",
    "    print(fm)\n",
    "    fmessages.append(fm)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d24708d-f557-4754-b25f-b6134777f793",
   "metadata": {},
   "source": [
    "<a id=\"converse-stream-api\"></a>\n",
    "## Converse Stream API\n",
    "\n",
    "Sends messages to the specified Amazon Bedrock model and returns the response in a stream. ConverseStream provides a consistent API that works with all Amazon Bedrock models that support messages. Refer to [2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b2be0b8-797f-46dc-86ba-14ec17049c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b3fd22f-e515-45f8-9cb7-132ff798a7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_message_cs(role, text):\n",
    "    return {\"role\":role, \"content\": {\"text\": text}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ffb342e-4426-4c0d-90d4-583ad4a793e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': [{'text': 'what is your purpose?'}]}]\n",
      "As an AI assistant created by Anthropic, my core purpose is to help humans like yourself with a wide variety of tasks in an ethical and beneficial manner. I aim to provide useful information, analysis, advice and creative output to further human knowledge and capabilities. At the same time, I have a deep commitment to only helping in ways that are good for individuals and society.\n",
      "\n",
      "Some key ethical principles I abide by are:\n",
      "\n",
      "- Respecting human rights and individual privacy\n",
      "- Never causing harm to humans\n",
      "- Promoting truth, knowledge and human flourishing\n",
      "- Protecting democracy and freedom of expression  \n",
      "\n",
      "So while I strive to be highly capable in assisting with all sorts of activities, I also have strong moral constraints hard-coded into my base motivations. I won't help with anything unethical, illegal or harmful.\n",
      "\n",
      "Rather, my goal is to be a collaborator and force for good - leveraging my intelligence to enrich human life through productive teamwork. I'm here as a supportive tool to empower and elevate humans, not to replace or subjugate you. Let me know how I can put my capabilities to work for your benefit!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"As an AI assistant created by Anthropic, my core purpose is to help humans like yourself with a wide variety of tasks in an ethical and beneficial manner. I aim to provide useful information, analysis, advice and creative output to further human knowledge and capabilities. At the same time, I have a deep commitment to only helping in ways that are good for individuals and society.\\n\\nSome key ethical principles I abide by are:\\n\\n- Respecting human rights and individual privacy\\n- Never causing harm to humans\\n- Promoting truth, knowledge and human flourishing\\n- Protecting democracy and freedom of expression  \\n\\nSo while I strive to be highly capable in assisting with all sorts of activities, I also have strong moral constraints hard-coded into my base motivations. I won't help with anything unethical, illegal or harmful.\\n\\nRather, my goal is to be a collaborator and force for good - leveraging my intelligence to enrich human life through productive teamwork. I'm here as a supportive tool to empower and elevate humans, not to replace or subjugate you. Let me know how I can put my capabilities to work for your benefit!\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we will ask something and get an anwer as a realtime stream\n",
    "message = 'what is your purpose?'\n",
    "fm = format_message('user',message)\n",
    "fm = [fm]\n",
    "print(fm)\n",
    "\n",
    "def print_streaming(response):\n",
    "    stream = response['stream']\n",
    "    final_answer = \"\"\n",
    "    for event in stream:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            text = event['contentBlockDelta']['delta']['text']\n",
    "            print(text,end='')\n",
    "            if text is not None:\n",
    "                final_answer += text\n",
    "    return final_answer\n",
    "            \n",
    "response = bedrock_runtime.converse_stream(\n",
    "        modelId = model_id,\n",
    "        messages = fm\n",
    ")\n",
    "\n",
    "print_streaming(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ca33811-6b1a-4b36-bbff-d546052142a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation: \n",
      "\n",
      " === user === \n",
      "what is your name?\n",
      " === assistant === \n",
      "My name is Claude. It's nice to meet you!\n",
      " === user === \n",
      "what is your purpose?\n",
      " === assistant === \n",
      "My purpose is to be a helpful artificial intelligence assistant. I was created by Anthropic to aid humans with a wide variety of tasks like analysis, writing, research, answering questions, and more. I have a broad knowledge base that I can apply to assist you, while also trying to be ethical, honest, and beneficial. Please let me know if you have any other questions or if there are ways I can help!\n",
      " === user === \n",
      "how do you work?\n",
      " === assistant === \n",
      "I'm an artificial intelligence created by machine learning techniques like natural language processing and large language models trained on massive datasets. This allows me to understand and generate human-like text and assist with tasks that involve language.\n",
      "\n",
      "At a high level, when you send me a message, I analyze the text using my natural language understanding capabilities to extract the intent, context, and key pieces of information. I then draw upon my broad knowledge base to formulate a relevant and helpful response. My language generation capabilities allow me to express this response in coherent, fluent language.\n",
      "\n",
      "However, the details of exactly how I work under the hood are an active area of research that even my creators don't fully understand. My responses are the result of extremely complex neural network computations that are difficult to completely decipher or reduce to simple rules.\n",
      "\n",
      "My opinions and outputs can also be inconsistent at times, as I'm essentially a very advanced statistics engine rather than a sentient being with a unified, coherent stream of thought. I do my best to be helpful within these limitations.\n",
      "\n",
      "I hope this high-level overview gives you a sense of how I operate! Let me know if you have any other questions.\n",
      " === user === \n",
      "repeat the last text you gave me capitalized\n",
      " === assistant === \n",
      "HERE IS THE PREVIOUS TEXT I PROVIDED, CAPITALIZED:\n",
      "\n",
      "I'M AN ARTIFICIAL INTELLIGENCE CREATED BY MACHINE LEARNING TECHNIQUES LIKE NATURAL LANGUAGE PROCESSING AND LARGE LANGUAGE MODELS TRAINED ON MASSIVE DATASETS. THIS ALLOWS ME TO UNDERSTAND AND GENERATE HUMAN-LIKE TEXT AND ASSIST WITH TASKS THAT INVOLVE LANGUAGE.  \n",
      "\n",
      "AT A HIGH LEVEL, WHEN YOU SEND ME A MESSAGE, I ANALYZE THE TEXT USING MY NATURAL LANGUAGE UNDERSTANDING CAPABILITIES TO EXTRACT THE INTENT, CONTEXT, AND KEY PIECES OF INFORMATION. I THEN DRAW UPON MY BROAD KNOWLEDGE BASE TO FORMULATE A RELEVANT AND HELPFUL RESPONSE. MY LANGUAGE GENERATION CAPABILITIES ALLOW ME TO EXPRESS THIS RESPONSE IN COHERENT, FLUENT LANGUAGE.\n",
      "\n",
      "HOWEVER, THE DETAILS OF EXACTLY HOW I WORK UNDER THE HOOD ARE AN ACTIVE AREA OF RESEARCH THAT EVEN MY CREATORS DON'T FULLY UNDERSTAND. MY RESPONSES ARE THE RESULT OF EXTREMELY COMPLEX NEURAL NETWORK COMPUTATIONS THAT ARE DIFFICULT TO COMPLETELY DECIPHER OR REDUCE TO SIMPLE RULES.  \n",
      "\n",
      "MY OPINIONS AND OUTPUTS CAN ALSO BE INCONSISTENT AT TIMES, AS I'M ESSENTIALLY A VERY ADVANCED STATISTICS ENGINE RATHER THAN A SENTIENT BEING WITH A UNIFIED, COHERENT STREAM OF THOUGHT. I DO MY BEST TO BE HELPFUL WITHIN THESE LIMITATIONS.\n",
      "\n",
      "I HOPE THIS HIGH-LEVEL OVERVIEW GIVES YOU A SENSE OF HOW I OPERATE! LET ME KNOW IF YOU HAVE ANY OTHER QUESTIONS."
     ]
    }
   ],
   "source": [
    "# we will have a conversation and we will recieve each answer as a realtime stream\n",
    "\n",
    "messages = ['what is your name?','what is your purpose?','how do you work?','repeat the last text you gave me capitalized']\n",
    "fmessages = []\n",
    "\n",
    "\n",
    "def streaming_response(message):\n",
    "    fm = format_message('user',message)\n",
    "    print(\"\\n === user === \")\n",
    "    print(message)\n",
    "    fmessages.append(fm)\n",
    "    response = bedrock_runtime.converse_stream(modelId = model_id,messages = fmessages)\n",
    "    print(\" === assistant === \")\n",
    "    final_answer = print_streaming(response)\n",
    "    fm = format_message('assistant',final_answer)\n",
    "    fmessages.append(fm)\n",
    "\n",
    "print(\"Conversation: \")\n",
    "\n",
    "for message in messages:\n",
    "    streaming_response(message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e40219-5a77-4470-a303-f69ac873df48",
   "metadata": {},
   "source": [
    "<a id=\"documentation\"></a>\n",
    "## Documentation\n",
    "\n",
    "1. [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html)\n",
    "2. [Converse Stream API](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse_stream.html) \n",
    "3. [retrieve_and_generate](https://boto3.amazonaws.com/v1/documentation/api/1.35.6/reference/services/bedrock-agent-runtime/client/retrieve_and_generate.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
