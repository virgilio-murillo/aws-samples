{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing with Custom Container for Anomaly Detection\n",
    "\n",
    "This notebook demonstrates how to run anomaly detection using SageMaker Processing with a custom Docker container. The solution implements region-level breach detection using the RRCF (Robust Random Cut Forest) algorithm.\n",
    "\n",
    "## Features\n",
    "- **Self-contained**: Generates synthetic data automatically\n",
    "- **Custom Docker Container**: Uses Python 3.10 with scientific computing libraries\n",
    "- **Anomaly Detection**: Implements RRCF algorithm for breach detection\n",
    "- **End-to-End Workflow**: From local testing to production deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "First, we'll generate synthetic data that matches the structure of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "def generate_synthetic_data(num_rows=6018, output_file='synthetic_data.parquet'):\n",
    "    \"\"\"Generate synthetic data matching the original parquet file structure\"\"\"\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    random.seed(42)\n",
    "    \n",
    "    # Define possible values based on original data\n",
    "    marketplace_sets = ['IN']\n",
    "    marketplace_ids = [44571, 44572, 44573, 44574, 44575]\n",
    "    sortabilities = ['sortable', 'non_conveyable']\n",
    "    iog_types = ['FBA', 'P3P_DF', 'PREMIUM_PRO_SELLER', 'ARIPL']\n",
    "    regions = ['BLR_CLUSTER', 'HYD_CLUSTER', 'KOL_CLUSTER', 'AMD_CLUSTER', 'HRA_CLUSTER', 'BOM_CLUSTER']\n",
    "    \n",
    "    # Generate date range - ensure we have data for the target date (2025-11-08)\n",
    "    target_date = datetime(2025, 11, 8)\n",
    "    start_date = target_date - timedelta(weeks=26)\n",
    "    date_range = pd.date_range(start=start_date, end=target_date, freq='W')\n",
    "    \n",
    "    # Ensure target date is included\n",
    "    if target_date not in date_range:\n",
    "        date_range = date_range.union([target_date])\n",
    "    \n",
    "    data = []\n",
    "    \n",
    "    # Generate combinations to ensure we have enough cohorts for the target date\n",
    "    cohort_combinations = []\n",
    "    for region in regions:\n",
    "        for sortability in sortabilities:\n",
    "            for iog_type in iog_types:\n",
    "                cohort_combinations.append((region, sortability, iog_type))\n",
    "    \n",
    "    # Generate data with emphasis on target date\n",
    "    target_date_rows = int(num_rows * 0.3)  # 30% of data for target date\n",
    "    historical_rows = num_rows - target_date_rows\n",
    "    \n",
    "    # Generate target date data first\n",
    "    for i in range(target_date_rows):\n",
    "        region, sortability, iog_type = random.choice(cohort_combinations)\n",
    "        \n",
    "        # Generate units with some correlation\n",
    "        total_units = np.random.randint(5000, 600000)\n",
    "        inregion_units = np.random.randint(1000, min(total_units, 400000))\n",
    "        \n",
    "        # Generate metric_new (appears to be a ratio/percentage)\n",
    "        metric_new = np.random.uniform(0.1, 0.95)\n",
    "        \n",
    "        row = {\n",
    "            'marketplace_set': 'IN',\n",
    "            'marketplace_id': random.choice(marketplace_ids),\n",
    "            'snapshot_week': target_date,\n",
    "            'sortability': sortability,\n",
    "            'iog_type': iog_type,\n",
    "            'destination_zip_assigned_region': region,\n",
    "            'inregion_units': inregion_units,\n",
    "            'total_units': total_units,\n",
    "            'metric_new': metric_new\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    # Generate historical data\n",
    "    for i in range(historical_rows):\n",
    "        region, sortability, iog_type = random.choice(cohort_combinations)\n",
    "        \n",
    "        # Generate units with some correlation\n",
    "        total_units = np.random.randint(1000, 600000)\n",
    "        inregion_units = np.random.randint(100, min(total_units, 400000))\n",
    "        \n",
    "        # Generate metric_new (appears to be a ratio/percentage)\n",
    "        metric_new = np.random.uniform(0.1, 0.95)\n",
    "        \n",
    "        row = {\n",
    "            'marketplace_set': 'IN',\n",
    "            'marketplace_id': random.choice(marketplace_ids),\n",
    "            'snapshot_week': random.choice(date_range[:-1]),  # Exclude target date\n",
    "            'sortability': sortability,\n",
    "            'iog_type': iog_type,\n",
    "            'destination_zip_assigned_region': region,\n",
    "            'inregion_units': inregion_units,\n",
    "            'total_units': total_units,\n",
    "            'metric_new': metric_new\n",
    "        }\n",
    "        data.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Ensure correct data types\n",
    "    df['marketplace_set'] = df['marketplace_set'].astype('object')\n",
    "    df['marketplace_id'] = df['marketplace_id'].astype('int64')\n",
    "    df['snapshot_week'] = pd.to_datetime(df['snapshot_week'])\n",
    "    df['sortability'] = df['sortability'].astype('object')\n",
    "    df['iog_type'] = df['iog_type'].astype('object')\n",
    "    df['destination_zip_assigned_region'] = df['destination_zip_assigned_region'].astype('object')\n",
    "    df['inregion_units'] = df['inregion_units'].astype('int64')\n",
    "    df['total_units'] = df['total_units'].astype('int64')\n",
    "    df['metric_new'] = df['metric_new'].astype('float64')\n",
    "    \n",
    "    # Save to parquet\n",
    "    df.to_parquet(output_file, index=False)\n",
    "    \n",
    "    print(f\"Generated synthetic data: {output_file}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Target date (2025-11-08) rows: {len(df[df['snapshot_week'] == target_date])}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate synthetic data\n",
    "df = generate_synthetic_data()\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Local Inference Testing\n",
    "\n",
    "Test the anomaly detection algorithm locally with the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# Install dependencies\n",
    "subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas', 'joblib', 'numpy', 'rrcf', 'scipy', 'pyarrow', 'swifter'], check=True)\n",
    "\n",
    "# Run the inference script\n",
    "subprocess.run([sys.executable, 'standalone_region_training_test.py', '--dataset_date', '2025-11-08', '--output_path', './output'], check=True)\n",
    "\n",
    "print(\"Inference completed successfully!\")\n",
    "print(f\"Output saved to: {os.path.abspath('./output')}\")\n",
    "\n",
    "# Display output file info\n",
    "output_files = os.listdir('./output')\n",
    "for file in output_files:\n",
    "    file_path = os.path.join('./output', file)\n",
    "    file_size = os.path.getsize(file_path)\n",
    "    print(f\"Generated file: {file} ({file_size} bytes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Results\n",
    "\n",
    "Load and examine the anomaly detection results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and display results\n",
    "df = pd.read_parquet('./output/iris_region_breach_output.parquet')\n",
    "\n",
    "print(f\"Results Summary:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Breaches detected: {df['breach'].sum()}/{len(df)} cohorts\")\n",
    "print(f\"Total impact: {df['Impact'].sum():.2f}\")\n",
    "\n",
    "print(f\"\\nBreach Status:\")\n",
    "print(df['breach'].value_counts())\n",
    "\n",
    "print(f\"\\nTop 5 cohorts by impact:\")\n",
    "print(df.nlargest(5, 'Impact')[['cohort', 'Impact', 'anomaly_score', 'breach']])\n",
    "\n",
    "print(f\"\\nBreached cohorts:\")\n",
    "breached = df[df['breach'] == 1]\n",
    "if len(breached) > 0:\n",
    "    print(breached[['cohort', 'IRIS', 'anomaly_score', 'Impact']])\n",
    "else:\n",
    "    print(\"No breaches detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build Docker Container\n",
    "\n",
    "Build the Docker container for SageMaker Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Docker container\n",
    "result = subprocess.run(['docker', 'buildx', 'build', '--platform', 'linux/amd64', '-t', 'sagemaker-processing-test-amd64', '.'], \n",
    "                       capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ Docker image built successfully!\")\n",
    "else:\n",
    "    print(\"❌ Docker build failed:\")\n",
    "    print(\"STDERR:\", result.stderr)\n",
    "    print(\"STDOUT:\", result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Test Docker Container Locally\n",
    "\n",
    "Test the Docker container with the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test directories\n",
    "os.makedirs('test-input', exist_ok=True)\n",
    "os.makedirs('test-output', exist_ok=True)\n",
    "\n",
    "# Copy synthetic data to test input\n",
    "import shutil\n",
    "shutil.copy(\"./synthetic_data.parquet\", \"./test-input/data.parquet\")\n",
    "\n",
    "# Run Docker container\n",
    "result = subprocess.run([\n",
    "    'docker', 'run', '--rm',\n",
    "    '-v', f'{os.getcwd()}/test-input:/opt/ml/processing/input',\n",
    "    '-v', f'{os.getcwd()}/test-output:/opt/ml/processing/output',\n",
    "    'sagemaker-processing-test-amd64',\n",
    "    '--dataset_date', '2025-11-08',\n",
    "    '--output_path', '/opt/ml/processing/output'\n",
    "], capture_output=True, text=True)\n",
    "\n",
    "print(\"Docker container execution:\")\n",
    "print(\"Exit code:\", result.returncode)\n",
    "if result.returncode == 0:\n",
    "    print(\"✅ Container ran successfully!\")\n",
    "    \n",
    "    # Check output\n",
    "    if os.path.exists('./test-output/iris_region_breach_output.parquet'):\n",
    "        df = pd.read_parquet('./test-output/iris_region_breach_output.parquet')\n",
    "        print(f\"Output shape: {df.shape}\")\n",
    "        print(f\"Breaches detected: {df['breach'].sum()}/{len(df)} cohorts\")\n",
    "        print(\"✅ Docker output file generated successfully!\")\n",
    "    else:\n",
    "        print(\"❌ Output file not found\")\n",
    "else:\n",
    "    print(\"❌ Container failed:\")\n",
    "    print(\"STDERR:\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Verify Docker Output\n",
    "\n",
    "Examine the output from the Docker container test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if output file exists from local Docker test\n",
    "output_file = './test-output/iris_region_breach_output.parquet'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_parquet(output_file)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nFirst 3 rows:\\n{df.head(3)}\")\n",
    "    print(f\"\\n✅ Docker output verified successfully!\")\n",
    "else:\n",
    "    print(\"Output file not found. Run the Docker test first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Deploy to SageMaker Processing\n",
    "\n",
    "Deploy the container to Amazon ECR and run a SageMaker Processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize clients\n",
    "ecr_client = boto3.client('ecr', region_name='us-west-2')\n",
    "s3_client = boto3.client('s3', region_name='us-west-2')\n",
    "sagemaker_client = boto3.client('sagemaker', region_name='us-west-2')\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Configuration\n",
    "timestamp = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "ecr_repo_name = 'sagemaker-processing-test'\n",
    "bucket_name = f'sagemaker-processing-bucket-{timestamp}'\n",
    "job_name = f'sagemaker-processing-job-{timestamp}'\n",
    "account_id = boto3.client('sts').get_caller_identity()['Account']\n",
    "image_uri = f'{account_id}.dkr.ecr.us-west-2.amazonaws.com/{ecr_repo_name}:amd64'\n",
    "\n",
    "print(f\"Starting SageMaker processing job: {job_name}\")\n",
    "\n",
    "# 1. Create ECR repository\n",
    "try:\n",
    "    ecr_client.create_repository(repositoryName=ecr_repo_name)\n",
    "    print(f\"✅ Created ECR repository: {ecr_repo_name}\")\n",
    "except ecr_client.exceptions.RepositoryAlreadyExistsException:\n",
    "    print(f\"✅ ECR repository already exists: {ecr_repo_name}\")\n",
    "\n",
    "# 2. Login and push Docker image to ECR\n",
    "subprocess.run(f'aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin {account_id}.dkr.ecr.us-west-2.amazonaws.com', shell=True, check=True)\n",
    "subprocess.run(f'docker tag sagemaker-processing-test-amd64:latest {image_uri}', shell=True, check=True)\n",
    "subprocess.run(f'docker push {image_uri}', shell=True, check=True)\n",
    "print(f\"✅ Pushed Docker image to ECR: {image_uri}\")\n",
    "\n",
    "# 3. Create S3 bucket\n",
    "s3_client.create_bucket(\n",
    "    Bucket=bucket_name,\n",
    "    CreateBucketConfiguration={'LocationConstraint': 'us-west-2'}\n",
    ")\n",
    "print(f\"✅ Created S3 bucket: {bucket_name}\")\n",
    "\n",
    "# 4. Upload input data\n",
    "s3_client.upload_file('./synthetic_data.parquet', bucket_name, 'input/data.parquet')\n",
    "print(f\"✅ Uploaded input data to S3\")\n",
    "\n",
    "# 5. Create/verify IAM role\n",
    "role_name = 'SageMakerExecutionRole'\n",
    "role_arn = f'arn:aws:iam::{account_id}:role/{role_name}'\n",
    "\n",
    "try:\n",
    "    iam_client.get_role(RoleName=role_name)\n",
    "    print(f\"✅ IAM role exists: {role_name}\")\n",
    "except iam_client.exceptions.NoSuchEntityException:\n",
    "    # Create role\n",
    "    trust_policy = {\n",
    "        \"Version\": \"2012-10-17\",\n",
    "        \"Statement\": [\n",
    "            {\n",
    "                \"Effect\": \"Allow\",\n",
    "                \"Principal\": {\"Service\": \"sagemaker.amazonaws.com\"},\n",
    "                \"Action\": \"sts:AssumeRole\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy)\n",
    "    )\n",
    "    \n",
    "    # Attach policies\n",
    "    policies = [\n",
    "        'arn:aws:iam::aws:policy/AmazonSageMakerFullAccess',\n",
    "        'arn:aws:iam::aws:policy/AmazonS3FullAccess'\n",
    "    ]\n",
    "    \n",
    "    for policy in policies:\n",
    "        iam_client.attach_role_policy(RoleName=role_name, PolicyArn=policy)\n",
    "    \n",
    "    print(f\"✅ Created IAM role: {role_name}\")\n",
    "    time.sleep(10)  # Wait for role propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create and Monitor SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Create processing job\n",
    "response = sagemaker_client.create_processing_job(\n",
    "    ProcessingJobName=job_name,\n",
    "    ProcessingResources={\n",
    "        'ClusterConfig': {\n",
    "            'InstanceCount': 1,\n",
    "            'InstanceType': 'ml.m5.large',\n",
    "            'VolumeSizeInGB': 30\n",
    "        }\n",
    "    },\n",
    "    AppSpecification={\n",
    "        'ImageUri': image_uri,\n",
    "        'ContainerArguments': ['--dataset_date', '2025-11-08', '--output_path', '/opt/ml/processing/output']\n",
    "    },\n",
    "    ProcessingInputs=[\n",
    "        {\n",
    "            'InputName': 'input-data',\n",
    "            'S3Input': {\n",
    "                'S3Uri': f's3://{bucket_name}/input/',\n",
    "                'LocalPath': '/opt/ml/processing/input',\n",
    "                'S3DataType': 'S3Prefix',\n",
    "                'S3InputMode': 'File'\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    ProcessingOutputConfig={\n",
    "        'Outputs': [\n",
    "            {\n",
    "                'OutputName': 'output-data',\n",
    "                'S3Output': {\n",
    "                    'S3Uri': f's3://{bucket_name}/output/',\n",
    "                    'LocalPath': '/opt/ml/processing/output',\n",
    "                    'S3UploadMode': 'EndOfJob'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    RoleArn=role_arn\n",
    ")\n",
    "\n",
    "print(f\"✅ Created processing job: {job_name}\")\n",
    "\n",
    "# 7. Wait for completion\n",
    "print(\"Waiting for processing job to complete...\")\n",
    "while True:\n",
    "    response = sagemaker_client.describe_processing_job(ProcessingJobName=job_name)\n",
    "    status = response['ProcessingJobStatus']\n",
    "    print(f\"Status: {status}\")\n",
    "    \n",
    "    if status in ['Completed', 'Failed', 'Stopped']:\n",
    "        break\n",
    "    \n",
    "    time.sleep(30)\n",
    "\n",
    "if status == 'Completed':\n",
    "    print(\"✅ Processing job completed successfully!\")\n",
    "    \n",
    "    # Download output\n",
    "    s3_client.download_file(bucket_name, 'output/iris_region_breach_output.parquet', 'sagemaker_output.parquet')\n",
    "    print(\"✅ Downloaded output file: sagemaker_output.parquet\")\n",
    "else:\n",
    "    print(f\"❌ Processing job failed with status: {status}\")\n",
    "    if 'FailureReason' in response:\n",
    "        print(f\"Failure reason: {response['FailureReason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Verify SageMaker Results\n",
    "\n",
    "Examine the output from the SageMaker Processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if output file exists from SageMaker processing job\n",
    "output_file = './sagemaker_output.parquet'\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    df = pd.read_parquet(output_file)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Data types:\\n{df.dtypes}\")\n",
    "    print(f\"\\nFirst 3 rows:\\n{df.head(3)}\")\n",
    "    print(f\"\\nBreaches detected: {df['breach'].sum()}/{len(df)} cohorts\")\n",
    "    print(f\"\\n✅ SageMaker processing completed successfully!\")\n",
    "else:\n",
    "    print(\"SageMaker output file not found. Run the processing job first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Clean up temporary files and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up temporary files\n",
    "import shutil\n",
    "\n",
    "# Remove test directories\n",
    "if os.path.exists('test-input'):\n",
    "    shutil.rmtree('test-input')\n",
    "    print(\"Removed: test-input directory\")\n",
    "\n",
    "if os.path.exists('test-output'):\n",
    "    shutil.rmtree('test-output')\n",
    "    print(\"Removed: test-output directory\")\n",
    "\n",
    "print(\"\\n✅ Cleanup completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
