{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0e6c36",
   "metadata": {},
   "source": [
    "# SageMaker unified studdio Pipelines Tutorial\n",
    "\n",
    "### !IMPORTANT please read this markdown cell carefully before proceeding with the complete notebook. \n",
    "\n",
    "This notebook demonstrates how to create and execute a SageMaker pipeline, making emphasis in the differences between sagemaker unified studio and regular sagemaker. including steps for data processing, model training, evaluation, model creation, batch transformation, model registration, and conditional execution. I provide a workflow to tag all the resources generated from a pipeline execution for proper visualization in sagemaker unified studio.\n",
    "\n",
    "## SageMaker Unified Studio\n",
    "\n",
    "SageMaker Unified Studio is a new feature that provides a consolidated interface for managing SageMaker resources. It is important to note that when running this notebook's first section within a SageMaker Unified Studio project's Jupyter notebook , the created resources are automatically tagged for easy access via the project's console.\n",
    "\n",
    "However, if you run this notebook outside of a SageMaker Unified Studio project (e.g., on your local machine or in a separate notebook instance), you won't see the pipeline and its resources in the Unified Studio console by default. To visualize these resources in the Unified Studio console, you need to manually tag them, as shown in the second section of this notebook.\n",
    "\n",
    "* Section 1: Creating and Executing the Pipeline\n",
    "* Section 2: Tagging Resources for visualization on SageMaker Unified Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d2ad5-65c4-41ea-9ef9-e25b9d5cbdda",
   "metadata": {},
   "source": [
    "\n",
    "## Prerequisites for Running This Notebook\n",
    "\n",
    "To successfully run this notebook, ensure the following prerequisites are met:\n",
    "\n",
    "- **AWS Account**: An active AWS account with access to Amazon SageMaker.\n",
    "- **SageMaker Role**: An IAM role with permissions for SageMaker, S3, and related services. The role must include policies like `AmazonSageMakerFullAccess` and `AmazonS3FullAccess`.\n",
    "- **SageMaker Unified Studio**: Access to SageMaker Unified Studio for visualization (optional for running the notebook but required for viewing resources in the console).\n",
    "- **Python Environment**: A Python environment (e.g., `conda_python3`) with the following packages installed:\n",
    "  - `boto3==1.38.35`\n",
    "  - `sagemaker==2.246.0` (version compatible with the notebook, e.g., 2.x)\n",
    "  - `pandas==2.2.3`\n",
    "  - `sklearn==1.6.1`\n",
    "  - `numpy==1.26.4`\n",
    "  - `xgboost==3.0.2`\n",
    "- **S3 Access**: Access to an S3 bucket for storing input data, model artifacts, and outputs.\n",
    "- **Notebook Environment**: Run this notebook in a SageMaker notebook instance, SageMaker Unified Studio Jupyter notebook, or a local environment with AWS credentials configured.\n",
    "- **AWS Region**: Ensure the region supports SageMaker and the specified instance types (e.g., `ml.m5.xlarge`). The notebook retrieves the region dynamically, but verify compatibility.\n",
    "- **Data Files**: The notebook downloads the Abalone dataset from public S3 buckets (`sagemaker-example-files-prod-<region>` and `sagemaker-servicecatalog-seedcode-<region>`). Ensure network access to these buckets.\n",
    "- **Tagging Permissions**: For Section 2, the IAM role must have permissions to add tags to SageMaker resources (`sagemaker:AddTags`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb457b",
   "metadata": {},
   "source": [
    "## Section 1: Creating and Executing the Pipeline\n",
    "\n",
    "In this section, we define and execute a SageMaker pipeline that performs the following steps:\n",
    "\n",
    "1. **Processing Step**: Preprocesses the input data.\n",
    "2. **Training Step**: Trains an XGBoost model using the preprocessed data.\n",
    "3. **Evaluation Step**: Evaluates the trained model using a test dataset.\n",
    "4. **Conditional Step**: Checks if the model's mean squared error (MSE) is below a specified threshold.\n",
    "   - If yes, proceeds to:\n",
    "     - **Register Model Step**: Registers the model in the SageMaker model registry.\n",
    "     - **Create Model Step**: Creates a SageMaker model.\n",
    "     - **Batch Transform Step**: Performs batch transformation using the created model.\n",
    "   - If no, fails the pipeline execution.\n",
    "\n",
    "This pipeline demonstrates a complete machine learning workflow, from data preprocessing to model deployment, with a conditional check to ensure model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c8376",
   "metadata": {},
   "source": [
    "### Utility Function\n",
    "\n",
    "We define a helper function to create unique names with timestamps for resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa0feb4c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def create_name_with_timestamp(prefix):\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    return f\"{prefix}{timestamp}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8461310",
   "metadata": {},
   "source": [
    "### Initial Setup\n",
    "\n",
    "We import necessary libraries and set up SageMaker sessions and roles required for the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f99f5307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = sagemaker.get_execution_role()\n",
    "pipeline_session = PipelineSession()\n",
    "default_bucket = sagemaker_session.default_bucket()\n",
    "model_package_group_name = create_name_with_timestamp(\"AbaloneModelPackageGroupName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b03400-a14e-4a1c-bb5e-2e21e2d0c231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import sklearn\n",
    "import xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f2481b-1750-484e-8b22-b9ab46a5d1f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38.35\n",
      "2.246.0\n",
      "2.2.3\n",
      "1.6.1\n",
      "1.26.4\n",
      "3.0.2\n"
     ]
    }
   ],
   "source": [
    "print(boto3.__version__)\n",
    "print(sagemaker.__version__)\n",
    "print(pandas.__version__)\n",
    "print(sklearn.__version__)\n",
    "print(numpy.__version__)\n",
    "print(xgboost.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a54c3",
   "metadata": {},
   "source": [
    "### Downloading and Uploading Data\n",
    "\n",
    "We download the Abalone dataset and upload it to an S3 bucket for use in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701b8b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data uploaded to: s3://sagemaker-us-east-1-794038231401/abalone/abalone-dataset.csv\n",
      "Batch data uploaded to: s3://sagemaker-us-east-1-794038231401/abalone/abalone-dataset-batch\n"
     ]
    }
   ],
   "source": [
    "# Create a local directory for data if it doesn't exist\n",
    "!mkdir -p data\n",
    "\n",
    "local_path = \"data/abalone-dataset.csv\"\n",
    "s3 = boto3.resource(\"s3\")\n",
    "s3.Bucket(f\"sagemaker-example-files-prod-{region}\").download_file(\n",
    "    \"datasets/tabular/uci_abalone/abalone.csv\", local_path\n",
    ")\n",
    "base_uri = f\"s3://{default_bucket}/abalone\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(f\"Input data uploaded to: {input_data_uri}\")\n",
    "\n",
    "local_path = \"data/abalone-dataset-batch\"\n",
    "s3.Bucket(f\"sagemaker-servicecatalog-seedcode-{region}\").download_file(\n",
    "    \"dataset/abalone-dataset-batch\", local_path\n",
    ")\n",
    "batch_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=local_path,\n",
    "    desired_s3_uri=base_uri,\n",
    ")\n",
    "print(f\"Batch data uploaded to: {batch_data_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0820df74",
   "metadata": {},
   "source": [
    "### Defining Pipeline Parameters\n",
    "\n",
    "Pipeline parameters allow us to pass different values to the pipeline each time it runs, making it flexible and reusable. Here, we define parameters for instance counts, types, approval status, data URIs, and evaluation threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3434491",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    "    ParameterFloat,\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "instance_type = ParameterString(name=\"TrainingInstanceType\", default_value=\"ml.m5.xlarge\")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")\n",
    "input_data = ParameterString(\n",
    "    name=\"InputData\",\n",
    "    default_value=input_data_uri,\n",
    ")\n",
    "batch_data = ParameterString(\n",
    "    name=\"BatchData\",\n",
    "    default_value=batch_data_uri,\n",
    ")\n",
    "mse_threshold = ParameterFloat(name=\"MseThreshold\", default_value=6.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e8c4f",
   "metadata": {},
   "source": [
    "### Preprocessing Script\n",
    "\n",
    "This script preprocesses the Abalone dataset, handling data cleaning, feature encoding, and splitting into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02629e8c-e21b-4dc6-821e-7f1bd2708710",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for code if it doesn't exist\n",
    "!mkdir -p code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e1fa8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/preprocessing.py\n",
    "import argparse\n",
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# Define column names since the CSV is headerless\n",
    "feature_columns_names = [\n",
    "    \"sex\",\n",
    "    \"length\",\n",
    "    \"diameter\",\n",
    "    \"height\",\n",
    "    \"whole_weight\",\n",
    "    \"shucked_weight\",\n",
    "    \"viscera_weight\",\n",
    "    \"shell_weight\",\n",
    "]\n",
    "label_column = \"rings\"\n",
    "\n",
    "feature_columns_dtype = {\n",
    "    \"sex\": str,\n",
    "    \"length\": np.float64,\n",
    "    \"diameter\": np.float64,\n",
    "    \"height\": np.float64,\n",
    "    \"whole_weight\": np.float64,\n",
    "    \"shucked_weight\": np.float64,\n",
    "    \"viscera_weight\": np.float64,\n",
    "    \"shell_weight\": np.float64,\n",
    "}\n",
    "label_column_dtype = {\"rings\": np.float64}\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    z = x.copy()\n",
    "    z.update(y)\n",
    "    return z\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_dir = \"/opt/ml/processing\"\n",
    "\n",
    "    df = pd.read_csv(\n",
    "        f\"{base_dir}/input/abalone-dataset.csv\",\n",
    "        header=None,\n",
    "        names=feature_columns_names + [label_column],\n",
    "        dtype=merge_two_dicts(feature_columns_dtype, label_column_dtype),\n",
    "    )\n",
    "    numeric_features = list(feature_columns_names)\n",
    "    numeric_features.remove(\"sex\")\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    "    )\n",
    "\n",
    "    categorical_features = [\"sex\"]\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"missing\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocess = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    y = df.pop(\"rings\")\n",
    "    X_pre = preprocess.fit_transform(df)\n",
    "    y_pre = y.to_numpy().reshape(len(y), 1)\n",
    "\n",
    "    X = np.concatenate((y_pre, X_pre), axis=1)\n",
    "\n",
    "    np.random.shuffle(X)\n",
    "    train, validation, test = np.split(X, [int(0.7 * len(X)), int(0.85 * len(X))])\n",
    "\n",
    "    pd.DataFrame(train).to_csv(f\"{base_dir}/train/train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(\n",
    "        f\"{base_dir}/validation/validation.csv\", header=False, index=False\n",
    "    )\n",
    "    pd.DataFrame(test).to_csv(f\"{base_dir}/test/test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccae4a93",
   "metadata": {},
   "source": [
    "### Step 1: Data Processing\n",
    "\n",
    "We use a SageMaker Processing job to preprocess the input data with the script above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f9e940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/sagemaker/workflow/pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "framework_version = \"1.2-1\"\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=framework_version,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=\"sklearn-abalone-process\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "processor_args = sklearn_processor.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(source=input_data, destination=\"/opt/ml/processing/input\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=\"code/preprocessing.py\",\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(name=\"AbaloneProcess\", step_args=processor_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b100a50a",
   "metadata": {},
   "source": [
    "### Step 2: Model Training\n",
    "\n",
    "We train an XGBoost model using the preprocessed training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a261f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "model_path = f\"s3://{default_bucket}/AbaloneTrain\"\n",
    "image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    ")\n",
    "xgb_train = Estimator(\n",
    "    image_uri=image_uri,\n",
    "    instance_type=instance_type,\n",
    "    instance_count=1,\n",
    "    output_path=model_path,\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "xgb_train.set_hyperparameters(\n",
    "    objective=\"reg:linear\",\n",
    "    num_round=50,\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.7,\n",
    ")\n",
    "\n",
    "train_args = xgb_train.fit(\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"train\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=step_process.properties.ProcessingOutputConfig.Outputs[\"validation\"].S3Output.S3Uri,\n",
    "            content_type=\"text/csv\",\n",
    "        ),\n",
    "    }\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "step_train = TrainingStep(name=\"AbaloneTrain\", step_args=train_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9ce4ea",
   "metadata": {},
   "source": [
    "### Step 3: Model Evaluation\n",
    "\n",
    "We evaluate the trained model using the test dataset and calculate the mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "779f66a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing code/evaluation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile code/evaluation.py\n",
    "import json\n",
    "import pathlib\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"/opt/ml/processing/model/model.tar.gz\"\n",
    "    with tarfile.open(model_path) as tar:\n",
    "        tar.extractall(path=\".\")\n",
    "\n",
    "    model = pickle.load(open(\"xgboost-model\", \"rb\"))\n",
    "\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "\n",
    "    y_test = df.iloc[:, 0].to_numpy()\n",
    "    df.drop(df.columns[0], axis=1, inplace=True)\n",
    "\n",
    "    X_test = xgboost.DMatrix(df.values)\n",
    "\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, predictions)\n",
    "    std = np.std(y_test - predictions)\n",
    "    report_dict = {\n",
    "        \"regression_metrics\": {\n",
    "            \"mse\": {\"value\": mse, \"standard_deviation\": std},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    output_dir = \"/opt/ml/processing/evaluation\"\n",
    "    pathlib.Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    evaluation_path = f\"{output_dir}/evaluation.json\"\n",
    "    with open(evaluation_path, \"w\") as f:\n",
    "        f.write(json.dumps(report_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ba7726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "script_eval = ScriptProcessor(\n",
    "    image_uri=image_uri,\n",
    "    command=[\"python3\"],\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"script-abalone-eval\",\n",
    "    role=role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "eval_args = script_eval.run(\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=step_process.properties.ProcessingOutputConfig.Outputs[\"test\"].S3Output.S3Uri,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"evaluation\", source=\"/opt/ml/processing/evaluation\"),\n",
    "    ],\n",
    "    code=\"code/evaluation.py\",\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\", output_name=\"evaluation\", path=\"evaluation.json\"\n",
    ")\n",
    "step_eval = ProcessingStep(\n",
    "    name=\"AbaloneEval\",\n",
    "    step_args=eval_args,\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020023d1",
   "metadata": {},
   "source": [
    "### Step 4: Create Model\n",
    "\n",
    "If the model passes the evaluation threshold, we create a SageMaker model from the trained artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348c9791",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.model import Model\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=pipeline_session,\n",
    "    role=role,\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.model_step import ModelStep\n",
    "\n",
    "step_create_model = ModelStep(\n",
    "    name=\"AbaloneCreateModel\",\n",
    "    step_args=model.create(instance_type=\"ml.m5.large\", accelerator_type=\"ml.eia1.medium\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7cd33a",
   "metadata": {},
   "source": [
    "### Step 5: Batch Transformation\n",
    "\n",
    "We perform batch transformation on the batch data using the created model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec31d4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.transformer import Transformer\n",
    "\n",
    "transformer = Transformer(\n",
    "    model_name=step_create_model.properties.ModelName,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    output_path=f\"s3://{default_bucket}/AbaloneTransform\",\n",
    ")\n",
    "\n",
    "from sagemaker.inputs import TransformInput\n",
    "from sagemaker.workflow.steps import TransformStep\n",
    "\n",
    "step_transform = TransformStep(\n",
    "    name=\"AbaloneTransform\", transformer=transformer, inputs=TransformInput(data=batch_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517cddea",
   "metadata": {},
   "source": [
    "### Step 6: Register Model\n",
    "\n",
    "We register the model in the SageMaker model registry for versioning and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7245284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.model_metrics import MetricsSource, ModelMetrics\n",
    "\n",
    "model_metrics = ModelMetrics(\n",
    "    model_statistics=MetricsSource(\n",
    "        s3_uri=\"{}/evaluation.json\".format(\n",
    "            step_eval.arguments[\"ProcessingOutputConfig\"][\"Outputs\"][0][\"S3Output\"][\"S3Uri\"]\n",
    "        ),\n",
    "        content_type=\"application/json\",\n",
    "    )\n",
    ")\n",
    "\n",
    "register_args = model.register(\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    "    model_metrics=model_metrics,\n",
    ")\n",
    "\n",
    "step_register = ModelStep(name=\"AbaloneRegisterModel\", step_args=register_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d4216",
   "metadata": {},
   "source": [
    "### Step 7: Conditional Step\n",
    "\n",
    "This step checks if the MSE is below the threshold. If yes, it proceeds with model registration, creation, and transformation; otherwise, it fails the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7ed50c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.fail_step import FailStep\n",
    "from sagemaker.workflow.functions import Join\n",
    "\n",
    "step_fail = FailStep(\n",
    "    name=\"AbaloneMSEFail\",\n",
    "    error_message=Join(on=\" \", values=[\"Execution failed due to MSE >\", mse_threshold]),\n",
    ")\n",
    "\n",
    "from sagemaker.workflow.conditions import ConditionLessThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "\n",
    "cond_lte = ConditionLessThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name=step_eval.name,\n",
    "        property_file=evaluation_report,\n",
    "        json_path=\"regression_metrics.mse.value\",\n",
    "    ),\n",
    "    right=mse_threshold,\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name=\"AbaloneMSECond\",\n",
    "    conditions=[cond_lte],\n",
    "    if_steps=[step_register, step_create_model, step_transform],\n",
    "    else_steps=[step_fail],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e37dc3b",
   "metadata": {},
   "source": [
    "### Defining and Executing the Pipeline\n",
    "\n",
    "We assemble all steps into a SageMaker pipeline, upsert it, and start an execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd259a52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TrainingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'TransformJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline definition: {'Version': '2020-12-01', 'Metadata': {}, 'Parameters': [{'Name': 'ProcessingInstanceCount', 'Type': 'Integer', 'DefaultValue': 1}, {'Name': 'TrainingInstanceType', 'Type': 'String', 'DefaultValue': 'ml.m5.xlarge'}, {'Name': 'ModelApprovalStatus', 'Type': 'String', 'DefaultValue': 'PendingManualApproval'}, {'Name': 'InputData', 'Type': 'String', 'DefaultValue': 's3://sagemaker-us-east-1-794038231401/abalone/abalone-dataset.csv'}, {'Name': 'BatchData', 'Type': 'String', 'DefaultValue': 's3://sagemaker-us-east-1-794038231401/abalone/abalone-dataset-batch'}, {'Name': 'MseThreshold', 'Type': 'Float', 'DefaultValue': 6.0}], 'PipelineExperimentConfig': {'ExperimentName': {'Get': 'Execution.PipelineName'}, 'TrialName': {'Get': 'Execution.PipelineExecutionId'}}, 'Steps': [{'Name': 'AbaloneProcess', 'Type': 'Processing', 'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge', 'InstanceCount': {'Get': 'Parameters.ProcessingInstanceCount'}, 'VolumeSizeInGB': 30}}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-scikit-learn:1.2-1-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/preprocessing.py']}, 'RoleArn': 'arn:aws:iam::794038231401:role/service-role/SageMaker-ExecutionRole-20250401T103257', 'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': {'Get': 'Parameters.InputData'}, 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-794038231401/AbalonePipeline20250628075021/code/21bef2723200fd932faac85f3b632ed8389be9441821dfe454c33d7868131573/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'train', 'AppManaged': False, 'S3Output': {'S3Uri': {'Std:Join': {'On': '/', 'Values': ['s3:/', 'sagemaker-us-east-1-794038231401', 'AbalonePipeline20250628075021', {'Get': 'Execution.PipelineExecutionId'}, 'AbaloneProcess', 'output', 'train']}}, 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'validation', 'AppManaged': False, 'S3Output': {'S3Uri': {'Std:Join': {'On': '/', 'Values': ['s3:/', 'sagemaker-us-east-1-794038231401', 'AbalonePipeline20250628075021', {'Get': 'Execution.PipelineExecutionId'}, 'AbaloneProcess', 'output', 'validation']}}, 'LocalPath': '/opt/ml/processing/validation', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test', 'AppManaged': False, 'S3Output': {'S3Uri': {'Std:Join': {'On': '/', 'Values': ['s3:/', 'sagemaker-us-east-1-794038231401', 'AbalonePipeline20250628075021', {'Get': 'Execution.PipelineExecutionId'}, 'AbaloneProcess', 'output', 'test']}}, 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}]}}}, {'Name': 'AbaloneTrain', 'Type': 'Training', 'Arguments': {'AlgorithmSpecification': {'TrainingInputMode': 'File', 'TrainingImage': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3'}, 'OutputDataConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-794038231401/AbaloneTrain'}, 'StoppingCondition': {'MaxRuntimeInSeconds': 86400}, 'ResourceConfig': {'VolumeSizeInGB': 30, 'InstanceCount': 1, 'InstanceType': {'Get': 'Parameters.TrainingInstanceType'}}, 'RoleArn': 'arn:aws:iam::794038231401:role/service-role/SageMaker-ExecutionRole-20250401T103257', 'InputDataConfig': [{'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.AbaloneProcess.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'train'}, {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': \"Steps.AbaloneProcess.ProcessingOutputConfig.Outputs['validation'].S3Output.S3Uri\"}, 'S3DataDistributionType': 'FullyReplicated'}}, 'ContentType': 'text/csv', 'ChannelName': 'validation'}], 'HyperParameters': {'objective': 'reg:linear', 'num_round': '50', 'max_depth': '5', 'eta': '0.2', 'gamma': '4', 'min_child_weight': '6', 'subsample': '0.7'}, 'DebugHookConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-794038231401/AbaloneTrain', 'CollectionConfigurations': []}, 'ProfilerConfig': {'S3OutputPath': 's3://sagemaker-us-east-1-794038231401/AbaloneTrain', 'DisableProfiler': False}}}, {'Name': 'AbaloneEval', 'Type': 'Processing', 'Arguments': {'ProcessingResources': {'ClusterConfig': {'InstanceType': 'ml.m5.xlarge', 'InstanceCount': 1, 'VolumeSizeInGB': 30}}, 'AppSpecification': {'ImageUri': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3', 'ContainerEntrypoint': ['python3', '/opt/ml/processing/input/code/evaluation.py']}, 'RoleArn': 'arn:aws:iam::794038231401:role/service-role/SageMaker-ExecutionRole-20250401T103257', 'ProcessingInputs': [{'InputName': 'input-1', 'AppManaged': False, 'S3Input': {'S3Uri': {'Get': 'Steps.AbaloneTrain.ModelArtifacts.S3ModelArtifacts'}, 'LocalPath': '/opt/ml/processing/model', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'AppManaged': False, 'S3Input': {'S3Uri': {'Get': \"Steps.AbaloneProcess.ProcessingOutputConfig.Outputs['test'].S3Output.S3Uri\"}, 'LocalPath': '/opt/ml/processing/test', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-794038231401/AbalonePipeline20250628075021/code/f53baf166c7c718e2dba767d007798586b5c2c5ec6535d3a36c11a3187ea4a5e/evaluation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'evaluation', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-794038231401/script-abalone-eval-2025-06-28-07-50-21-345/output/evaluation', 'LocalPath': '/opt/ml/processing/evaluation', 'S3UploadMode': 'EndOfJob'}}]}}, 'PropertyFiles': [{'PropertyFileName': 'EvaluationReport', 'OutputName': 'evaluation', 'FilePath': 'evaluation.json'}]}, {'Name': 'AbaloneMSECond', 'Type': 'Condition', 'Arguments': {'Conditions': [{'Type': 'LessThanOrEqualTo', 'LeftValue': {'Std:JsonGet': {'PropertyFile': {'Get': 'Steps.AbaloneEval.PropertyFiles.EvaluationReport'}, 'Path': 'regression_metrics.mse.value'}}, 'RightValue': {'Get': 'Parameters.MseThreshold'}}], 'IfSteps': [{'Name': 'AbaloneRegisterModel-RegisterModel', 'Type': 'RegisterModel', 'Arguments': {'ModelPackageGroupName': 'AbaloneModelPackageGroupName20250628075019', 'ModelMetrics': {'ModelQuality': {'Statistics': {'ContentType': 'application/json', 'S3Uri': 's3://sagemaker-us-east-1-794038231401/script-abalone-eval-2025-06-28-07-50-21-345/output/evaluation/evaluation.json'}}, 'Bias': {}, 'Explainability': {}}, 'InferenceSpecification': {'Containers': [{'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3', 'Environment': {}, 'ModelDataUrl': {'Get': 'Steps.AbaloneTrain.ModelArtifacts.S3ModelArtifacts'}}], 'SupportedContentTypes': ['text/csv'], 'SupportedResponseMIMETypes': ['text/csv'], 'SupportedRealtimeInferenceInstanceTypes': ['ml.t2.medium', 'ml.m5.xlarge'], 'SupportedTransformInstanceTypes': ['ml.m5.xlarge']}, 'ModelApprovalStatus': {'Get': 'Parameters.ModelApprovalStatus'}, 'SkipModelValidation': 'None'}}, {'Name': 'AbaloneCreateModel-CreateModel', 'Type': 'Model', 'Arguments': {'ExecutionRoleArn': 'arn:aws:iam::794038231401:role/service-role/SageMaker-ExecutionRole-20250401T103257', 'PrimaryContainer': {'Image': '683313688378.dkr.ecr.us-east-1.amazonaws.com/sagemaker-xgboost:1.0-1-cpu-py3', 'Environment': {}, 'ModelDataUrl': {'Get': 'Steps.AbaloneTrain.ModelArtifacts.S3ModelArtifacts'}}}}, {'Name': 'AbaloneTransform', 'Type': 'Transform', 'Arguments': {'ModelName': {'Get': 'Steps.AbaloneCreateModel-CreateModel.ModelName'}, 'TransformInput': {'DataSource': {'S3DataSource': {'S3DataType': 'S3Prefix', 'S3Uri': {'Get': 'Parameters.BatchData'}}}}, 'TransformOutput': {'S3OutputPath': 's3://sagemaker-us-east-1-794038231401/AbaloneTransform'}, 'TransformResources': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge'}}}], 'ElseSteps': [{'Name': 'AbaloneMSEFail', 'Type': 'Fail', 'Arguments': {'ErrorMessage': {'Std:Join': {'On': ' ', 'Values': ['Execution failed due to MSE >', {'Get': 'Parameters.MseThreshold'}]}}}}]}}]}\n",
      "Pipeline ARN: arn:aws:sagemaker:us-east-1:794038231401:pipeline/AbalonePipeline20250628075021\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "pipeline_name = create_name_with_timestamp(\"AbalonePipeline\")\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        processing_instance_count,\n",
    "        instance_type,\n",
    "        model_approval_status,\n",
    "        input_data,\n",
    "        batch_data,\n",
    "        mse_threshold,\n",
    "    ],\n",
    "    steps=[step_process, step_train, step_eval, step_cond],\n",
    ")\n",
    "\n",
    "import json\n",
    "\n",
    "definition = json.loads(pipeline.definition())\n",
    "print(\"Pipeline definition:\", definition)\n",
    "\n",
    "pipeline_arn = pipeline.upsert(role_arn=role)['PipelineArn']\n",
    "print(f\"Pipeline ARN: {pipeline_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4491748",
   "metadata": {},
   "source": [
    "## Section 2: Tagging Resources for visualization on SageMaker Unified Studio\n",
    "\n",
    "To visualize the pipeline and its resources in the SageMaker Unified Studio console when running outside a Unified Studio project, we manually tag them. This section:\n",
    "\n",
    "1. Tags the pipeline.\n",
    "2. Starts a pipeline execution.\n",
    "3. Retrieves ARNs of created resources.\n",
    "4. Tags these resources for visibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bff5cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tags added to pipeline: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': 'af9ff4d1-f0a1-47d1-a06e-af8985146f6a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'af9ff4d1-f0a1-47d1-a06e-af8985146f6a', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 07:50:22 GMT'}, 'RetryAttempts': 0}}\n",
      "Pipeline execution started with ARN: arn:aws:sagemaker:us-east-1:794038231401:pipeline/AbalonePipeline20250628075021/execution/5snd76p7s48z\n",
      "\n",
      "Tagging TransformJob with ARN: arn:aws:sagemaker:us-east-1:794038231401:transform-job/pipelines-5snd76p7s48z-AbaloneTransform-58yi5S06bm\n",
      "Tags added: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': 'efc69632-9dfc-4df6-b7bd-67ca1531e71b', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'efc69632-9dfc-4df6-b7bd-67ca1531e71b', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 08:02:55 GMT'}, 'RetryAttempts': 0}}\n",
      "\n",
      "Tagging Model with ARN: arn:aws:sagemaker:us-east-1:794038231401:model/pipelines-5snd76p7s48z-AbaloneCreateModel-C-Vqloyw2M9P\n",
      "Tags added: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': '2cfec8fb-9cf7-48c6-82c8-ad0c4ab1147e', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '2cfec8fb-9cf7-48c6-82c8-ad0c4ab1147e', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 08:02:55 GMT'}, 'RetryAttempts': 0}}\n",
      "\n",
      "Tagging ProcessingJob with ARN: arn:aws:sagemaker:us-east-1:794038231401:processing-job/pipelines-5snd76p7s48z-AbaloneProcess-3YmkY86kkT\n",
      "Tags added: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': 'f0698886-b473-4d17-bf1e-3ed4d7c97299', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'f0698886-b473-4d17-bf1e-3ed4d7c97299', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 08:02:55 GMT'}, 'RetryAttempts': 0}}\n",
      "\n",
      "Tagging TrainingJob with ARN: arn:aws:sagemaker:us-east-1:794038231401:training-job/pipelines-5snd76p7s48z-AbaloneTrain-lQdGH3K602\n",
      "Tags added: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': '3a9e6bc4-9115-459c-8116-5afcd90a98a6', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '3a9e6bc4-9115-459c-8116-5afcd90a98a6', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 08:02:55 GMT'}, 'RetryAttempts': 0}}\n",
      "\n",
      "Tagging ModelPackageGroup with ARN: arn:aws:sagemaker:us-east-1:794038231401:model-package-group/AbaloneModelPackageGroupName20250628075019\n",
      "Tags added: {'Tags': [{'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'}, {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'}], 'ResponseMetadata': {'RequestId': 'c0de783c-5276-4425-aaed-ccd2e265fd56', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'c0de783c-5276-4425-aaed-ccd2e265fd56', 'content-type': 'application/x-amz-json-1.1', 'content-length': '127', 'date': 'Sat, 28 Jun 2025 08:02:55 GMT'}, 'RetryAttempts': 0}}\n"
     ]
    }
   ],
   "source": [
    "tags = [\n",
    "    {'Key': 'AmazonDataZoneDomain', 'Value': 'dzd_amkwelbodmx2av'},\n",
    "    {'Key': 'AmazonDataZoneProject', 'Value': '55v4yp8pzuqs6f'},\n",
    "]\n",
    "\n",
    "client = boto3.client('sagemaker')\n",
    "\n",
    "response = client.add_tags(\n",
    "    ResourceArn=pipeline_arn,\n",
    "    Tags=tags\n",
    ")\n",
    "print(f\"Tags added to pipeline: {response}\")\n",
    "\n",
    "# Start pipeline execution and wait for completion\n",
    "execution = pipeline.start()\n",
    "print(f\"Pipeline execution started with ARN: {execution.arn}\")\n",
    "execution.wait()\n",
    "\n",
    "execution_arn = execution.arn\n",
    "\n",
    "# Function to retrieve resource ARNs from pipeline execution\n",
    "def get_pipeline_resource_arns(execution_arn):\n",
    "    sm = boto3.client('sagemaker')\n",
    "    try:\n",
    "        response = sm.list_pipeline_execution_steps(PipelineExecutionArn=execution_arn)\n",
    "        steps = response['PipelineExecutionSteps']\n",
    "        arns = []\n",
    "        \n",
    "        for step in steps:\n",
    "            metadata = step['Metadata']\n",
    "            step_name = step['StepName']\n",
    "            if 'ProcessingJob' in metadata:\n",
    "                arns.append({'Step': step_name, 'Type': 'ProcessingJob', 'ARN': metadata['ProcessingJob']['Arn']})\n",
    "            elif 'TrainingJob' in metadata:\n",
    "                arns.append({'Step': step_name, 'Type': 'TrainingJob', 'ARN': metadata['TrainingJob']['Arn']})\n",
    "            elif 'Model' in metadata:\n",
    "                arns.append({'Step': step_name, 'Type': 'Model', 'ARN': metadata['Model']['Arn']})\n",
    "            elif 'RegisterModel' in metadata:\n",
    "                arns.append({'Step': step_name, 'Type': 'ModelPackage', 'ARN': metadata['RegisterModel']['Arn']})\n",
    "            elif 'TransformJob' in metadata:\n",
    "                arns.append({'Step': step_name, 'Type': 'TransformJob', 'ARN': metadata['TransformJob']['Arn']})\n",
    "        \n",
    "        return arns\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving pipeline steps: {e}\")\n",
    "        return []\n",
    "\n",
    "resource_arns = get_pipeline_resource_arns(execution_arn)\n",
    "resources = {arn['Type']: arn['ARN'] for arn in resource_arns}\n",
    "\n",
    "# Retrieve and tag Model Package Group ARN\n",
    "if 'ModelPackage' in resources:\n",
    "    model_package_arn = resources['ModelPackage']\n",
    "    response = client.describe_model_package(ModelPackageName=model_package_arn)\n",
    "    model_package_group_name = response['ModelPackageGroupName']\n",
    "    response = client.describe_model_package_group(ModelPackageGroupName=model_package_group_name)\n",
    "    resources['ModelPackageGroup'] = response['ModelPackageGroupArn']\n",
    "    resources.pop('ModelPackage')  # Remove individual ModelPackage as we tag the group\n",
    "\n",
    "# Tag all retrieved resources\n",
    "for resource_type, arn in resources.items():\n",
    "    print(f\"\\nTagging {resource_type} with ARN: {arn}\")\n",
    "    response = client.add_tags(\n",
    "        ResourceArn=arn,\n",
    "        Tags=tags\n",
    "    )\n",
    "    print(f\"Tags added: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae3d43",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have:\n",
    "\n",
    "1. Defined and executed a SageMaker pipeline that automates a machine learning workflow from data preprocessing to model deployment.\n",
    "2. Demonstrated how to tag the pipeline and its resources to make them visible in the SageMaker Unified Studio console when running outside a Unified Studio project.\n",
    "\n",
    "To view the pipeline and its executions, navigate to the SageMaker Unified Studio console and select the appropriate project. Ensure resources are tagged as shown."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
